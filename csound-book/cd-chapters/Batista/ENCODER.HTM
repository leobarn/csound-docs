<html>	<head>		<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">		<meta name="generator" content="Adobe GoLive 4">		<title>Neural Processing using Csound [ Encoder ]</title>		<meta name="Template" content="C:\PROGRAMAS\MICROSOFT OFFICE\OFFICE\HTML.DOT">	</head>	<body link="blue" vlink="purple" bgcolor="white">		<font size="4"><i><b>Testing RProp with a tight-encoder</b></i></font>		<p>A tight-encoder is a network that has the same input and output size, and a single (smaller) hidden layer to provide the coding. We train the net with the same pattern present at both the input and the output, so the net learns auto-associations of the given patterns. These will consist of binary strings, each with a single 1 and remaining zeros (first pattern is 10000, second is 01000, third is 00100, etc). We will use a network of size 10, so we'll be training ten of this patterns. Riedmiller describes his results with a 10-5-10 network (ten input cells, five hidden cells, ten output cells), but I could only get it to converge with 6 hidden neurons, so we'll use a 10-6-10 network. The initialization of the weights plays a big role, and the rnd() function is perhaps not random enough (a random generator with a <i>seed</i> value, would allow us to test several start weights, and then maybe we can - on average - reach the described results).</p>		<p>This network will <i>encode</i> the 10 patterns using the values of its weights. Since the number of weights does not grow, this gives a way of performing data compression, which is the more effective, the fewer hidden neurons are used. Riedmiller describes 12-2-12 encoders, converging within an average 322 epochs (iterations) using RProp, where plain backprop didnt find a solution at all. You can try it with this instrument, although I admit I havent been able to recreate exactly the published results.</p>		<p>An interesting (and conclusive) exercise, is to compare this much more complicated problem and the speed of convergence using RProp, with our back-prop testing of the (much simpler) XOR problem. Backprop went way past 2,000 iterations, while with RProp, the average needed iterations lies well below a hundred, and if a solution doesnt appear in the, say, 500 first epochs, you might as well give up. Its this robustness of RProp, that leads me to think it can be used, in the highly demanding world of sound synthesis and processing.</p>		<p>		<hr>		</p>		<p><b>The training code</b></p>		<p>The training process consists of two main steps:</p>		<ol>			<li>a <b>forward pass</b>, where the input pattern is presented to the net, and the response from that pattern is forwarded through all layers until the output (in our example, through the hidden and then through the output layer)			<li>a <b>backward pass</b>, where the error for each connection is determined, starting from the output layer, where the difference between the actual (from the forward pass) and the desired target values is used to calculate it, and proceeding backwards, propagating the error values through the net to the preceding layers, in a similar fashion to the way the activation was propagated forward through the net, only backwards, and using the already computed error values from the next layer.		</ol>		<p>Since I accepted the limitation of having just a single hidden layer, and also for clarity, I wasnt too worried in performing all layers using the same code loop. Thats why I process each layer in a separate loop during this instrument. At the end, the weight-update code shows what had to be done to compact this into a single main loop, running for each individual layer. For a multiple hidden layer network, most of this code could be used, but some caution must be used in the array accessing.</p>		<p>Lets see the code then, first of all the activation code, or forward-pass code:</p>		<p>We start by calculating the activation of all neurons in the hidden layer. For each neuron gather all inputs (<i>io</i>), times their connection weights (<i>iw</i>); then compute the sigmoid function for this added value (<i>inet</i>), and store it in the activation array (the 'ziw' of the <i>ia</i> value).</p>		<p><font face="Courier New" size="2">;activation of hidden layer<br>		ineu1 = 0<br>		loop2:<br>		inet = 0<br>		ineu0 = 0<br>		loop3:<br>		io zir i_pat_i+ineu0<br>		iw zir (i_w)+ineu0*ihsz+ineu1<br>		inet = inet+iw*io<br>		ineu0 = ineu0+1<br>		if (ineu0&lt;iisz) igoto loop3<br>		ia = 1/(1+exp(-inet))<br>		ziw ia, (i_a)+ineu1<br>		ineu1 = ineu1+1<br>		if (ineu1&lt;ihsz) igoto loop2<br>		<br>		</font>Then we calculate the activation of all neurons in the output layer, in a similar fashion, using the activation values of the neurons in the preceding (hidden) layer as the input values (<i>io</i>). Again, the added activation value (<i>inet</i>) is computed using the sigmoid, and the result stored in the activation array. But now we also check to see how well the net is performing (I know, we just started, but remember this code will be run many times, in an iterative process). For each output neuron, we compare the actual output value we just calculated, with the target, desired value, from the output pattern. If the distance between these two values is bigger than the predefined allowed error (ierng), then the neuron is not 'hitting' the desired spot, and we have what can be called 'a miss'. We use a variable <i>imiss</i> to count the number of badly recognized neurons for <b>all</b> patterns. Training stops when this value reaches zero, at the end of an iteration (meaning all patterns were entirely recognized).</p>		<p><font face="Courier New" size="2">;activation of output layer<br>		ineu1 = 0<br>		loop4:<br>		inet = 0<br>		ineu0 = 0<br>		loop5:<br>		io zir (i_a)+ineu0<br>		iw zir (i_w+iisz*ihsz)+ineu0*iosz+ineu1<br>		inet = inet+iw*io<br>		ineu0 = ineu0+1<br>		if (ineu0&lt;ihsz) igoto loop5<br>		ia = 1/(1+exp(-inet))<br>		ziw ia, (i_a+ihsz)+ineu1<br>		it zir i_pat_t+ineu1<br>		if (abs(ia-it)&lt;ierng) igoto yes<br>		imiss = imiss+1<br>		yes:<br>		ineu1 = ineu1+1<br>		if (ineu1&lt;iosz) igoto loop4</font></p>		<p><font face="Courier New" size="2"><br>		</font>Now that we performed the forward pass, which updated the activation values for the whole net, we need to determine the error at the output, and transfer it backwards from the output layer, to the hidden layer. This backward pass, is the calculation of the error gradient, that we'll be needing as part of the learning rule. The actual formulas were introduced in the preceding section, and you may want to compare the code with what is presented there. Basically we want to calculate the <i>dedw</i> values, which will require the previous knowledge of the <i>dedo</i> values.</p>		<p>For the output layer, the dE/do value depends only on the difference between the desired and actual states. For each input connection of the neurons in this layer, the dE/dw value is then updated proportionally to the dE/do determined value, the derivative of the activation, and the input value on the particular connection.<font face="Courier New" size="2"><br>		<br>		;===============for the output layer=================<br>		ineuk = 0<br>		loopk0:<br>		it zir i_pat_t+ineuk<br>		ia zir (i_a+ihsz)+ineuk<br>		idedo = -(it-ia)<br>		ziw idedo, (i_dedo+ihsz)+ineuk<br>		ineuj = 0<br>		loopj0:<br>		io zir (i_a)+ineuj<br>		idedw zir (i_dedw+iisz*ihsz)+ineuj*iosz+ineuk<br>		idedw = idedw+idedo*(ia*(1-ia))*io<br>		ziw idedw, (i_dedw+iisz*ihsz)+ineuj*iosz+ineuk<br>		ineuj = ineuj+1<br>		if (ineuj&lt;ihsz) igoto loopj0<br>		ineuk = ineuk+1<br>		if (ineuk&lt;iosz) igoto loopk0</font></p>		<p><font face="Courier New" size="2"><br>		</font>For the hidden layer, the dE/do values found for the output layer are back propagated, through their respective connections, to each hidden neuron, resulting in a combined error value. The dE/dw value for each connection of each hidden neuron, is then updated in pretty much the same way as for the output layer, using the calculated dE/do's.</p>		<p><font face="Courier New" size="2">;===============for the hidden layer=================<br>		ineui = 0<br>		loopi:<br>		io zir i_pat_i+ineui<br>		ineuj = 0<br>		loopj:<br>		isoma = 0<br>		ineuk = 0<br>		loopk:<br>		idedo zir (i_dedo+ihsz)+ineuk<br>		iw zir (i_w+iisz*ihsz)+ineuj*iosz+ineuk<br>		ia zir (i_a+ihsz)+ineuk<br>		isoma = isoma+idedo*(ia*(1-ia))*iw<br>		ineuk = ineuk+1<br>		if (ineuk&lt;iosz) igoto loopk<br>		idedo = isoma<br>		ziw idedo, (i_dedo)+ineuj<br>		ia zir (i_a)+ineuj<br>		idedw zir (i_dedw)+ineui*ihsz+ineuj<br>		idedw = idedw+idedo*(ia*(1-ia))*io<br>		ineuj = ineuj+1<br>		if (ineuj&lt;ihsz) igoto loopj<br>		ineui = ineui+1<br>		if (ineui&lt;iisz) igoto loopi<br>		<br>		</font>This completes the main processing loop for a single pattern. We will repeat these same steps for each and every pattern in the training set, and only after that we will perform any corrections to the weights. Notice the dE/dw values, which are needed for the weight changes, are being accumulated over all patterns.</p>		<p>We increment our pattern-counting variable (<i>ipat</i>), and restart processing:</p>		<p><font face="Courier New" size="2">ipat = ipat+1<br>		if (ipat&lt;inump) igoto newpat</font></p>		<p>After all patterns, and if the <i>imiss</i> variable still is zero (meaning there were no errors), we can stop training</p>		<p><font face="Courier New" size="2">if (imiss==0) igoto done</font></p>		<p>Otherwise, its time to update the weights, using the RProp algorithm (I leave the explanation of this portion to the curiosity of the readers; Riedmiller's papers are easily found on the net, and his explanations very clear).</p>		<p>Here's the complete orchestra file : <a href="ENCODER.ORC">ENCODER.ORC</a></p>		<p>And the usual dummy score : <a href="DUMMY.SCO">DUMMY.SCO</a></p>		<p>&nbsp;</p>		<p>		<hr>		</p>		<dir>			<p><b>Next : </b><a href="CUSTOM.HTM">A custom neural instrument</a></p>			<p><b>Previous : </b><a href="RPROP.HTM">Resilient back-propagation (RPROP)</a></p>			<p><b>Up:</b> <a href="index.html">Back to index</a>		</dir>	</body></html>