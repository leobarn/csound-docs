<html>	<head>		<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">		<meta name="generator" content="Adobe GoLive 4">		<title>Neural Processing using Csound [ XOR ]</title>		<meta name="Template" content="C:\PROGRAMAS\MICROSOFT OFFICE\OFFICE\HTML.DOT">	</head>	<body link="blue" vlink="purple" bgcolor="white">		<font size="4"><i><b>Testing the Back-Prop algorithm with the XOR problem</b></i></font>		<p>In this section, we'll be constructing a soundless instrument, which will merely help us consolidate our knowledge of the back-prop algorithm. We will be implementing a solution for the xor problem, using simply i-code and print isntructions to display the net's output.</p>		<p>Lets first consider the topology of the net we'll be needing. It will have two inputs, and a single output. The hidden layer size will be two cells, which is enough for solving this particular problem (so I've heard :)):</p>		<center>			<p><img src="IMG6.GIF" width="473" height="221"></p>		</center>		<center>			<dir>				<p>&nbsp;</p>			</center>			<p>The picture also shows the variables we'll be needing to store values:</p>		</dir>		<p>		<table cellspacing="0" border="0" cellpadding="4" width="599">			<tr>				<td width="56%" valign="top">i [1], i [2]</td>				<td width="44%" valign="top">The input pattern area</td>			</tr>			<tr>				<td width="56%" valign="top">o [1]</td>				<td width="44%" valign="top">The target pattern area&nbsp;</td>			</tr>			<tr>				<td width="56%" valign="top">a [1][1], a [1][2]</td>				<td width="44%" valign="top">Activation values for hidden layer&nbsp;</td>			</tr>			<tr>				<td width="56%" valign="top">a [2][1]</td>				<td width="44%" valign="top">Activation values for output layer&nbsp;</td>			</tr>			<tr>				<td width="56%" valign="top">w [1][1][1], w [1][1][2], w [1][2][1],w [1][2][2]</td>				<td width="44%" valign="top">Weight values for the input connections of hidden layer</td>			</tr>			<tr>				<td width="56%" valign="top">w [2][1][1], w [2][2][1]</td>				<td width="44%" valign="top">Weight values for input connections of output layer</td>			</tr>			<tr>				<td width="56%" valign="top">d [1][1], d [1][2]</td>				<td width="44%" valign="top">Delta for hidden layer&nbsp;</td>			</tr>			<tr>				<td width="56%" valign="top">d [2][1]</td>				<td width="44%" valign="top">Delta for output layer&nbsp;</td>			</tr>		</table>		</p>		<div align="right">			<dir>				<p>&nbsp;</p>			</dir></div>		<p>Since this is a very small network, and to make things clear, I will use single variables for each of these values, isntead of some zak-space array. The nomenclature used for the variable names, will consist of the variable letter (A, W or D), followed by a number identifying the respective layer, followed by an underscore, followed by the indexes that identify each value within the layer.</p>		<p>Examples:</p>		<p>i_a1_2 will be the activation value of cell 2 in layer 1 (the hidden layer)</p>		<p>i_w2_21 will be the connection weight between cell 2 in layer 1 (the hidden layer) and cell 1 in layer 2 (the output layer)</p>		<p>i_i2 will be input value number 2</p>		<p>i_o1 will be the output (target) value</p>		<p>&nbsp;</p>		<p>Lets inspect the code then. First the usual sr/kr definition (not relevant in this case since all is done is i-time processing)</p>		<p><font face="Courier New" size="2">sr = 44100<br>		kr = 441<br>		ksmps = 100<br>		<br>		</font>Next, the start of the instrument definition, where some parameters are defined.</p>		<p>The learning rate eta, which will determine the scaling factor by which weights are updated. Lets keep it low for now, but you may increase this value later (up to 1, and even 2 or 3), to see how the algorithm behaves (you wont be able to increase it much, before the algorithm starts diverging...).</p>		<p><font face="Courier New" size="2">ieta = .25 ;training coeficient</font></p>		<p>The number of patterns to be trained (since we'll be training a xor table, this corresponds to the four possible combinations of the two binary inputs)</p>		<p><font face="Courier New" size="2">inump = 4 ;number of patterns</font></p>		<p>for the exclusive-OR table:</p>		<p>		<table cellspacing="0" border="0" cellpadding="4" width="235">			<tr>				<td width="24%" valign="top">					<center>						<b>I1</b></center>				</td>				<td width="24%" valign="top">					<center>						<b>I2</b></center>				</td>				<td width="52%" valign="top">					<center>						<b>O1</b></center>				</td>			</tr>			<tr>				<td width="24%" valign="top">					<center>						0</center>				</td>				<td width="24%" valign="top">					<center>						0</center>				</td>				<td width="52%" valign="top">					<center>						0 xor 0 = 0</center>				</td>			</tr>			<tr>				<td width="24%" valign="top">					<center>						0</center>				</td>				<td width="24%" valign="top">					<center>						1</center>				</td>				<td width="52%" valign="top">					<center>						0 xor 1 = 1</center>				</td>			</tr>			<tr>				<td width="24%" valign="top">					<center>						1</center>				</td>				<td width="24%" valign="top">					<center>						0</center>				</td>				<td width="52%" valign="top">					<center>						1 xor 0 = 1</center>				</td>			</tr>			<tr>				<td width="24%" valign="top">					<center>						1</center>				</td>				<td width="24%" valign="top">					<center>						1</center>				</td>				<td width="52%" valign="top">					<center>						1 xor 1 = 0</center>				</td>			</tr>		</table>		</p>		<p>&nbsp;</p>		<p>And also the maximum number of iterations (in case the net diverges, this is a maximum value of epochs; if at this point the net hasnt found a solution, you might as well give up) :</p>		<p><font face="Courier New" size="2">imaxit = 50000 ;maximum number of iterations</font></p>		<p>This value is way overestimated. This particular implementation reaches the solution at 2490 iterations, within a 40% range. Its certainly not like we're making things hard for it... Maybe 10,000 iterations would be an ideal number, but sadly many solutions (when there are solutions) go past that.</p>		<p>Next the whole bunch of variables is initialized with zero (which wasnt really necessary, except for the weights which need be initialized, but serves as presentation for the nomenclature).</p>		<p>Just be sure to notice the weight array initialization. It seems best to initialize the weights with random values, which is whats being done here, with a value in the range 0 to 1 (1/100 to 100/100 to be precise).</p>		<p><font face="Courier New" size="2">;the activation array<br>		i_a1_1 = 0<br>		i_a1_2 = 0<br>		i_a2_1 = 0<br>		<br>		;the delta array<br>		i_d1_1 = 0<br>		i_d1_2 = 0<br>		i_d2_1 = 0<br>		<br>		;the weight array<br>		i_w1_11 = (rnd(99)+1)/100<br>		i_w1_12 = (rnd(99)+1)/100<br>		i_w1_21 = (rnd(99)+1)/100<br>		i_w1_22 = (rnd(99)+1)/100<br>		i_w2_11 = (rnd(99)+1)/100<br>		i_w2_21 = (rnd(99)+1)/100<br>		<br>		;the input array<br>		i_i1 = 0<br>		i_i2 = 0<br>		<br>		;the output array<br>		i_o1 = 0<br>		<br>		</font>Now we'll start the iteration loop. The variable iter holds the present iteration number, and the loop restarts at the newitr: label.</p>		<p><font face="Courier New" size="2">;iteration loop<br>		iter = 0<br>		newitr:<br>		<br>		</font>This next variable ihit is used to determine if the solution has been reached. I start it with 1 in each iteration (in boolean style, a value of 'true'), and after each pattern training I turn it to 0 if the pattern is not correctly recognized. If I manage to still arrive with the value of 1 to the end of an iteration, it means all patterns were successfully recognized, and the net already found a solution.</p>		<p><font face="Courier New" size="2">ihit = 1</font></p>		<p>Inside the iteration loop, we have the pattern loop, which will, one by one, present each pattern pair to the net (all four of them for each <i>epoch</i>) and update the weights. Again a restart label newpat: is used for the looping:</p>		<p><font face="Courier New" size="2">ipat = 0<br>		newpat:</font></p>		<p>For the actual training, we must start by loading the input/output variables with the pattern corresponding to the current pattern number. This is straight forward with the following code:</p>		<p><font face="Courier New" size="2">;load input/output patterns<br>		if (ipat&gt;0) igoto skp0<br>		i_i1 = 0<br>		i_i2 = 0<br>		i_o1 = 0<br>		igoto skp3<br>		skp0:<br>		if (ipat&gt;1) igoto skp1<br>		i_i1 = 0<br>		i_i2 = 1<br>		i_o1 = 1<br>		igoto skp3<br>		skp1:<br>		if (ipat&gt;2) igoto skp2<br>		i_i1 = 1<br>		i_i2 = 0<br>		i_o1 = 1<br>		igoto skp3<br>		skp2:<br>		if (ipat&gt;3) igoto skp3<br>		i_i1 = 1<br>		i_i2 = 1<br>		i_o1 = 0<br>		skp3:<br>		<br>		</font>Then the output of the net is calculated, layer by layer, form the input to the output (the temporary variable inet). For each cell, we first multiply each weight by the stimulus through that particular weight and add the combined result. Then, the activation amount is updated using the formula for the logistic function:</p>		<p><font face="Courier New" size="2">;activation of neuron 1 from layer 1<br>		inet = i_i1*i_w1_11+i_i2*i_w1_21<br>		i_a1_1 = 1/(1+exp(-inet)))<br>		<br>		;activation of neuron 2 from layer 1<br>		inet = i_i1*i_w1_12+i_i2*i_w1_22<br>		i_a1_2 = 1/(1+exp(-inet)))</font></p>		<p>Once all neurons in the first layer have been computed, proceed with next layer:</p>		<p><font face="Courier New" size="2">;activation of neuron 1 from layer 2<br>		inet = i_a1_1*i_w2_11+i_a1_2*i_w2_21<br>		i_a2_1 = 1/(1+exp(-inet)))<br>		<br>		</font>At this point, we have our output neuron (i_a2_1) <i>firing</i> the output of the net, for the pattern that is being trained (i_i1 and i_i2). What we want to do is display the result, so that we can see the net evolving, during the instrument's execution. This is done simply with a print instruction (notice the variable itimes, the number of iterations to go by before printing, since we dont need to see the results at every single iteration):</p>		<p><font face="Courier New" size="2">;print output of the net<br>		itimes = 100; every 100 times<br>		if (frac(iter/itimes)!=0) igoto prskp<br>		print iter, ipat, i_i1, i_i2, i_a2_1<br>		prskp:</font></p>		<p>And we must check if the net has found the solution. The irange variable holds the error percentage that we'll be allowing. Since we're dealing with binary patterns, we may find that a value above .6 is already a sufficient approximation to 1, a goal which wont demand as many iterations, as to get within, say, 10% of the result. You may later try smaller values for irange, to realize for yourself the number of required iterations.</p>		<p>If the distance between the actual and desired output is less than the maximum error percentage, then it means the pattern was successfully recognized. Otherwise zero ihit, so that the program knows training isnt over yet.</p>		<p><font face="Courier New" size="2">;check if output within range<br>		irange = .4<br>		idst = abs(i_a2_1-i_o1)<br>		if (idst&lt;irange) igoto yes<br>		ihit = 0; no<br>		yes:<br>		<br>		<br>		</font>If this was the last pattern, and the ihit value is still 1 (meaning all patterns were within the specified range from the target), then we can stop training, since we already found a solution</p>		<p><font face="Courier New" size="2">if (ipat&lt;3) igoto sk1<br>		if (ihit==1) igoto done<br>		sk1:<br>		<br>		</font>Otherwise, lets correct the weights using the back-propagation rule:</p>		<p>First lets find delta for the output layer. This will consist of the difference between the actual and desired outputs, multiplied by the derivative of the activation function, <i>out*(1-out)</i>, where <i>out</i> is the activation level of the neuron.</p>		<p><font face="Courier New" size="2">;find delta for output layer<br>		i_d2_1 = (i_o1-i_a2_1)*(i_a2_1*(1-i_a2_1))</font></p>		<p>Then, lets backpropagate this delta to the hidden layer. The back propagated amount is stored in ibp, and as you see it consists of the found delta for the output layer, times the respective connection weights to each cell in the hidden layer. This backpropagated value, is then multiplied by the activation function derivative, as before:</p>		<p><font face="Courier New" size="2">;delta for cell 1<br>		ibp = i_d2_1*i_w2_11<br>		i_d1_1 = ibp*(i_a1_1*(1-i_a1_1))<br>		<br>		;delta for cell 2<br>		ibp = i_d2_1*i_w2_21<br>		i_d1_2 = ibp*(i_a1_2*(1-i_a1_2))<br>		<br>		</font>We're finally able to correct all the connection weights, based on the just found deltas. For each connection, the change in weight value equals the connection's input activation (activation level of the cell on the input of the connection), times the delta value for the cell on the output of the connection, times the learning rate eta.</p>		<p>Heres the code for the four connections of the hidden layer, plus the two connections of the output layer:</p>		<p><font face="Courier New" size="2">;layer 1 connections<br>		i_w1_11 = i_w1_11+ieta*i_d1_1*i_i1<br>		i_w1_12 = i_w1_12+ieta*i_d1_2*i_i1<br>		i_w1_21 = i_w1_21+ieta*i_d1_1*i_i2<br>		i_w1_22 = i_w1_22+ieta*i_d1_2*i_i2<br>		<br>		;layer 2 connections<br>		i_w2_11 = i_w2_11+ieta*i_d2_1*i_a1_1<br>		i_w2_21 = i_w2_21+ieta*i_d2_1*i_a1_2</font></p>		<p>At this point, we've trained one pattern, and should proceed with the next one. Thats whats done by</p>		<p><font face="Courier New" size="2">ipat = ipat+1<br>		if (ipat&lt;inump) igoto newpat</font></p>		<p>After all four patterns have been used, it looks like we ended an iteration cycle. So lets just increment the iteration counter, and loop to the next epoch (unless the maximum number of iterations was reached, which means the net diverged)</p>		<p><font face="Courier New" size="2">iter = iter+1<br>		if (iter&lt;imaxit) igoto newitr<br>		<br>		</font>If eventually the net manages to successfully recognize all four patterns during an iteration, this is where program control will be transferred to:</p>		<p><font face="Courier New" size="2">done:</font></p>		<p>All we have to do now, is print the number of iterations, and the result solution. This solution will consist simply of the weight values, that you can use to check for ourselves, they in fact solve the problem</p>		<p><font face="Courier New" size="2">;print down results<br>		print iter<br>		print i_w1_11<br>		print i_w1_12<br>		print i_w1_21<br>		print i_w1_22<br>		print i_w2_11<br>		print i_w2_21</font></p>		<p>And thats all (no sound on this one, sorry :))</p>		<p><font face="Courier New" size="2">endin</font>&nbsp;</p>		<p>&nbsp;</p>		<p>Here's the complete orchestra file : <a href="BPXOR.ORC">BPXOR.ORC</a></p>		<p>The score just needs to call the instrument with whatever duration. You can still just use this <a href="DUMMY.SCO">DUMMY.SCO</a><font face="Courier New" size="2"><b>.</b></font></p>		<p>		<hr>		</p>		<dir>			<p><b>Next : </b><a href="RPROP.HTM">Resilient back-propagation (RPROP)</a></p>			<p><b>Previous : </b><a href="BACKPROP.HTM">Hidden layer training (back-propagation)</a></p>			<p><b>Up:</b> <a href="index.html">Back to index</a>		</dir>	</body></html>