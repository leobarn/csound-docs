<html>	<head>		<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">		<meta name="generator" content="Adobe GoLive 4">		<title>Neural Processing using Csound [ RProp ]</title>		<meta name="Version" content="8.0.3612">		<meta name="Date" content="2/24/97">		<meta name="Template" content="C:\PROGRAMAS\MICROSOFT OFFICE\OFFICE\HTML.DOT">	</head>	<body text="black" link="blue" vlink="purple" bgcolor="white">		<font size="4"><i><b>Resilient Back-Propagation</b></i></font>		<p>To overcome the limitations of the classical gradient descent technique, many variations on the back-prop algorithm have been proposed. Some of the most well known are Quickprop, SuperSAB and RProp. All this methods present a reduction on the number of iterations needed to complete the task, as opposed to plain back-propagation, and manage to succeed in tasks where plain backprop fails. The most effective approaches use adaptive techniques, to correct the algorithm as the training proceeds. While backprop was originally a non-evolving gradient descent technique, where all customizing was decided at the beginning and then left unchanged during the training, this made it rather stiff when dealing with situations that required a more refined treatment. Consequently, adaptive methods that dynamically corrected the learning parameters (for instance the learning rate, the neurons activation thresholds, or the error derivative size, among others), started to emerge. One of my favorites is RProp, due to its simplicity, and powerful results.</p>		<p>As you recall, backpropagation tries to minimize the error gradient, by correcting the network weights, over consecutive iterations. At each iteration, the weight correction leaps the gradient to another value, which might close down to a minimum that solves the problem, but can also leap over the minimum, if the leap size is too big (due to the size of the error derivative being too big), and miss the solution.</p>		<p>To solve this problem, Martin Riedmiller proposed an adaptive method, where at each iteration, only the gradient sign is taken into account to determine the direction in which the network should evolve. Furthermore, each individual weight change amount is determined dynamically, and changes as a result of the learning process. Notice that the weight change amount, is independent of the gradient value, which is only used to calculate the direction of change for that weight.</p>		<p>I wont discuss the intricacies of the algorithm here (its rather simple, and you can get a describing paper from Riedmiller himself through the links section), but in words, what's being done is this: at each iteration, after all patterns have been presented to the net, and the error gradient for each weight has been computed, we compare each weight's gradient, with the preceding (from the previous iteration) gradient. If the derivative has changed its sign, then this means the minimum was leaped over, due to a too big step, so we must go back to the last iteration and try again with a smaller step. If on the contrary, the derivative has not changed its sign, then this means the algorithm is going down the hill, and the weight change amount should be increased, to speed up convergence.</p>		<p><font size="4"><i><b>Computing the Gradient</b></i></font></p>		<p>Now, we'll be focusing on the gradient calculation code. In the backprop section we used a <i>delta</i> variable, since we didnt need separate <i><b>dE/do</b></i> and <i><b>dE/dw</b></i> values. For RProp we'll be needing the <i><b>dE/dw</b></i> values, which in turn need <i><b>dE/do</b></i> to be calculated, so we must compute them separately into their own arrays that you'll see below. We will also need another array for each weight's current increment value, as well as another for each weight's previous gradient value.</p>		<p>From now on we'll be considering a network with a single hidden layer. This may seem restrictive, but I think its a reasonable compromise, between network performance and processing time. Besides, most conventional functions do not need any extra hidden layers, so we'll be prepared for most problems.</p>		<p>Lets use the formulas for the calculation of the partial error derivatives, to write the gradient determination code (using informal C style). We'll focus on the procedure for a single pattern, not forgetting this process must be repeated for all patterns in an epoch.</p>		<p>&nbsp;</p>		<p><i><b>Constants:</b></i></p>		<p>These determine the network topology,</p>		<p>		<table cellspacing="0" border="0" cellpadding="4" width="311">			<tr>				<td width="36%" valign="top">INSIZE</td>				<td width="64%" valign="top">number of input cells</td>			</tr>			<tr>				<td width="36%" valign="top">HIDSIZE</td>				<td width="64%" valign="top">number of hidden cells</td>			</tr>			<tr>				<td width="36%" valign="top">OUTSIZE</td>				<td width="64%" valign="top">number of output cells</td>			</tr>		</table>		</p>		<p>&nbsp;</p>		<p><i><b>Arrays:</b></i></p>		<p>These are the array variables we'll be needing to store the network values,</p>		<p>		<table cellspacing="0" border="0" cellpadding="4" width="682">			<tr>				<td width="19%" valign="top">a[L][I]</td>				<td width="81%" valign="top">activation of cell I in layer L</td>			</tr>			<tr>				<td width="19%" valign="top">w[L][I][J]</td>				<td width="81%" valign="top">weight of connection between cell I in layer L-1 (preceding layer) to cell J in layer L</td>			</tr>			<tr>				<td width="19%" valign="top">dedo[L][I]</td>				<td width="81%" valign="top">dE/do for cell I in layer L</td>			</tr>			<tr>				<td width="19%" valign="top">dedw[L][I][J]</td>				<td width="81%" valign="top">dE/dw for weight of connection between cell I in layer L-1 to cell J in layer L</td>			</tr>			<tr>				<td width="19%" valign="top">input[I]</td>				<td width="81%" valign="top">value number I of input pattern</td>			</tr>			<tr>				<td width="19%" valign="top">target[I]</td>				<td width="81%" valign="top">value number I of target pattern</td>			</tr>		</table>		</p>		<div align="right">			<dir>				<p>&nbsp;</p>			</dir></div>		<p>For simplicity, in this code skeleton, the layer index will be 'HID' when referring to the hidden layer, and 'OUT' when referring to the output layer. Since we'll be using the zak system later on, and also for clarity, all array values will be placed in local variables prior to computing result values.</p>		<p>One final remark, since the activation function is considered sigmoid, all the <i>f'(net) </i>terms will resolve to <i>out(1-out)</i>.&nbsp;</p>		<dir>			<p>&nbsp;</p>		</dir>		<p><i><b>Compute Gradient for output layer</b></i></p>		<p>		<table cellspacing="0" border="0" cellpadding="4" width="636">			<tr>				<td valign="top" colspan="3">for(k=0; k&lt;OUTSIZE; ++k) {</td>				<td width="35%" valign="top">&nbsp;</td>			</tr>			<tr>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">tk = target[k]</td>				<td width="35%" valign="middle" rowspan="3"><img src="FORM16.GIF" width="171" height="59"></td>			</tr>			<tr>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">ok = a[OUT][k]</td>			</tr>			<tr>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">dedo[OUT][k] = -(tk-ok)</td>			</tr>			<tr>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">for(j=0; j&lt;HIDSIZE; ++j) {</td>				<td width="35%" valign="middle" rowspan="6"><img src="FORM15.GIF" width="214" height="60"></td>			</tr>			<tr>				<td width="7%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="51%" valign="top">dedok = dedo[OUT][k]</td>			</tr>			<tr>				<td width="7%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="51%" valign="top">outk = a[OUT][k]</td>			</tr>			<tr>				<td width="7%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="51%" valign="top">oj = a[HID][j]</td>			</tr>			<tr>				<td width="7%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="51%" valign="top">dedw[OUT][j][k] += dedok*(outk*(1-outk))*oj</td>			</tr>			<tr>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">}</td>			</tr>			<tr>				<td valign="top" colspan="3">}</td>				<td width="35%" valign="top">&nbsp;</td>			</tr>		</table>		</p>		<p>&nbsp;</p>		<p><i><b>Compute Gradient for hidden layer</b></i></p>		<p>		<table cellspacing="0" border="0" cellpadding="4" width="656">			<tr>				<td valign="top" colspan="4">for(i=0; i&lt;INSIZE; i++) {</td>				<td width="40%" valign="top">&nbsp;</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td valign="top" colspan="3">for(j=0; j&lt;HIDSIZE; j++) {</td>				<td width="40%" valign="top">&nbsp;</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">bp = 0</td>				<td width="40%" valign="middle" rowspan="8"><img src="FORM18.GIF" width="235" height="60"></td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">for(k=0; k&lt;OUTSIZE; ++k) {</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="39%" valign="top">dedok = dedo[OUT][k]</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="39%" valign="top">wjk = w[OUT][j][k]</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="39%" valign="top">outk = a[OUT][k]</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td width="39%" valign="top">bp += dedok*(outk*(1-outk))*wjk</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">}</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">dedo[HID][j] = bp</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">outj = a[HID][j]</td>				<td width="40%" valign="middle" rowspan="4"><img src="FORM17.GIF" width="207" height="60"></td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">oi = input[i]</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">dedoj = dedo[HID][j]</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td width="7%" valign="top">&nbsp;</td>				<td valign="top" colspan="2">dedw[HID][i][j] += dedoj*(outj*(1-outj))*oi</td>			</tr>			<tr>				<td width="6%" valign="top">&nbsp;</td>				<td valign="top" colspan="3">}</td>				<td width="40%" valign="top">&nbsp;</td>			</tr>			<tr>				<td valign="top" colspan="4">}</td>				<td width="40%" valign="top">&nbsp;</td>			</tr>		</table>		</p>		<dir>			<p>&nbsp;</p>		</dir>		<p>So, after this is done, the accumulated error gradient for this last pattern, is placed in the <i>dedw</i> array. We are now ready to transport this algorithm to Csound language, and apply the RProp weight correction scheme. That's what we'll do in the next section.</p>		<p>&nbsp;</p>		<p>		<hr>		</p>		<dir>			<p><b>Next : </b><a href="ENCODER.HTM">Testing RProp with a tight encoder</a></p>			<p><b>Previous : </b><a href="BPXOR.HTM">Testing back-prop with the XOR problem</a></p>			<p><b>Up:</b> <a href="index.html">Back to index</a>		</dir>	</body></html>