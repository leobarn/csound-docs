<html>	<head>		<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">		<meta name="generator" content="Adobe GoLive 4">		<title>Neural Processing using Csound [ BAM ]</title>		<meta name="Template" content="C:\PROGRAMAS\MICROSOFT OFFICE\OFFICE\HTML.DOT">	</head>	<body link="blue" vlink="purple" bgcolor="white">		<font size="4"><i><b>An associative memory instrument</b></i></font>		<p>So, we should be able by now of implementing a Bidirectional Associative Memory (BAM), using the hebbian learning rule described previously. To do so, we must think what will the training patterns be, or in other words, what do we want the net to learn.</p>		<p>One possible implementation is to have the network learn sample values, provided for by a number of audio samples. What we'll do, then, is to build a simple pattern recognizer, which will take audio files as the training patterns, and that will allow us afterwards, to test it using one of the learned patterns, or even an unknown one.</p>		<p>To be able to learn a sample file, we must train the net using one sample at a time. So basically, our net will be presented with a single sample in its input, and another single sample in its output. These sample patterns will then be learned by the net using the hebbian rule. Finally, another stimulus sample will be presented to the net, and the response of the net to that stimulus will be our resulting sample, the one that will actually be output.</p>		<p>What I'm actually talking about, is building an instrument that for each control pass, trains a single layer network with up to three pattern pairs, and finds the response of the net to an additional stimulus sample. You can imagine, the computational stress this will put on your machines, if you use this:</p>		<p><font face="Courier New" size="2">sr = 44100<br>		kr = 44100<br>		ksmps = 1</font></p>		<p>One way of speeding up the performance of this instrument, is to decrease the control rate. &nbsp;The neural net is being trained at k-rate, so if we lower this rate, we'll be getting a downsampled version of the original sound samples, being used in the training. The result is obviously not so detailed, but its a compromise. In a perfect environment, we should leave ksmps at 1, so that there is no difference between sr and kr, but values up to ksmps = 10 are still satisfactory, while decreasing the compiling time to almost a tenth. Lowering even more the control rate, say to 100 ksmps, you get all sorts of weird effects (some of them cool :)).</p>		<p>Since we'll be using zak space to hold both the sample values for the training patterns, and the data arrays for the network, we need to allocate it at program start. Just count the amount of zak k-rate positions you need (16 for the in array + 16 for the out array + 16*16 for the weight array), and update it here. Since zero is not allowed, we need to define 1 for the a-space, although no a-space is needed.</p>		<p><font face="Courier New" size="2">zakinit 1, 288</font></p>		<p>Now lets start by inspecting the initial constants we'll be needing for the instrument. First there's the number of patterns (inump). This variable just provides a way of making the program loops start at a pattern number less than the maximum of 3. If you want just two patterns trained, decrease this value to 2.</p>		<p><font face="Courier New" size="2">inump = 3</font></p>		<p>For the array data storage, I think the best solution is to keep each array start position in a i-variable, and use this i-variable, plus an offset k-variable to determine the position of a single cell within the array. This makes accessing each position of the array very simple. If the array start is iarray, and the offset value is koffset, you just have to read or write from position iarray+koffset, using an offset value in the range <i>0</i> to <i>size-1</i>.</p>		<p>To implement this kind of network we need three arrays. The input array, where the input sample in binary form will be placed, the output array, which does the same for the output sample, and the weight array, holding the weights for all the connections. The input/output arrays will be 16 positions each, since we'll be using 16 bit patterns. The weight array has the size equal to the input size times the output size, which in this case is 16*16=256.</p>		<p>Here are the start position holders for each of the three arrays:</p>		<p><font face="Courier New" size="2">ibin = 1; [1-16]<br>		ibout = 17; [17-32]<br>		ipes = 33; [33-288]</font></p>		<p>Lets move on to the actual audio part. You could use any method for the a-signal generation, but I choose to use soundin for simplicity. For example, input pattern number 1 is loaded with the following code:</p>		<p><font face="Courier New" size="2">ain1 soundin &quot;c:\csound\samples\soundin.1&quot;<br>		ksigi1 downsamp ain1</font></p>		<p>And for the remaining patterns the same method is used (I choose the first 3 standard sample files, merely by convenience; please try your weirdest stuff :)) :</p>		<p><font face="Courier New" size="2">ain2 soundin &quot;c:\csound\samples\soundin.2&quot;<br>		ksigi2 downsamp ain2<br>		<br>		ain3 soundin &quot;c:\csound\samples\soundin.3&quot;<br>		ksigi3 downsamp ain3</font></p>		<p>An even better way, is to have a p-field here in place of the soundin parameter filename, and simply change the soundin number(s) from the score (you will need 3+3+1 soundin numbers being passed as p-fields).</p>		<p>Since the BAM is only called at k-time, we need to downsample the signal to k-time. This downsampling allows you to spare on the computation, but if you want to preserve all the original definition, you should try running this with the control rate equal to the sample rate (sr = kr, ksmps = 1).</p>		<p>You must provide as many training pattern pairs in the ksigi1/ksigo1, ksigi2/ksigo2, ksigi3/ksigo3 variables, as the number you defined in inump. Additionally, you must provide for another signal kstim, which will be the stimulus signal the trained net is fed with. All this variables will be getting some downsampled signal, but you can generate this signal all sorts of ways. Since this is just an introductory example, I start by using one same signal for each input/output pair. Furthermore, I will only be asking the net (using the stimulus pattern) for the previously learned patterns, but I could stimulate the net with whichever sample I could come up with. The net will hopefully respond with something close to the pattern that bares more resemblance to the stimulus.</p>		<p>So for simplicity, the stimulus will be either ksigi1, ksigi2 or ksigi3</p>		<p><font face="Courier New" size="2">kstim = ksigi1</font></p>		<p>&nbsp;</p>		<p>Another important point deals with the range of values the net will have to learn. Since the training patterns will be samples, this represent values in the range of -32768 to +32767 (assuming 16 bit samples). As I said previously, single neurons cant be expected to deal with such wide ranges of values. One possibility would be to scale the value down to a more appropriate range, but this would mean loosing audio definition. Another possibility is to encode the audio in some way, splitting the value across a group of neurons, instead of a single one.</p>		<p>One very basic approach is to simply calculate the 16 bit value that corresponds to the sample integer value, and present this value to the net, one bit per neuron. This is a convenient solution, since our neurons will be binary in this first instrument. But this method has a serious disadvantage, that stems from the fact that binary number for 10000, although being next to 01111, results in very different patterns for the net. This means that if the net mistakes just one bit, and if that bit is in one of the most significant positions (binary digits to the left), the resulting audio value will be very far from the intended one. Nevertheless, this method is very easily implemented, and we'll stick with this far from perfect solution for now.</p>		<p>A possible improvement would be to use a Gray code, to overcome this limitation. But Gray codes do not eliminate the problem, as a bit in error may still lead to wide deviations. Nevertheless, you may want to check the <a href="GRAY.ORC">GRAY.ORC</a> example, where the basic algorithm for coding binary to Gray, and decoding from Gray back to binary, is presented.</p>		<p>&nbsp;</p>		<p><b>Now, we're ready to start training.</b></p>		<p>As our first step, we need to initialize the weight array. This is done once at the start of each k-pass, thus clearing the array for each k-rate sample that is going to be learned. The following code implements that, zeroing the weight array. The math involved is that for finding the offset within the array, from a two dimensional position (i,j). It is easily proven that the offset of value at position (i, j) is ( i * 16 + j ). You can check that below:</p>		<p><font face="Courier New" size="2">kndi = 0<br>		loopi:<br>		kndj = 0<br>		loopj:<br>		zkw 0, ipes+kndi*16+kndj<br>		kndj = kndj+1<br>		if (kndj&lt;16) kgoto loopj<br>		kndi = kndi+1<br>		if (kndi&lt;16) kgoto loopi</font></p>		<p>Since there will be three patterns, being run by the same code, and since I didnt want to repeat the code three times (trust me, its not a good programming practice) I use the same code for all three patterns. Its easily done using a pair of variables (ksigi and ksigo) to hold the current input and output samples within the loop. Another variable kpass holds the actual pattern that is being processed. The main code will be executed for each incremental value of this variable, ranging from 1 to a maximum of 3 (unless inump is lower than 3, in which case inump will be the upper limit). The restrt: label sets the point well the training cycle will restart, for each pattern.</p>		<p><font face="Courier New" size="2">kpass = 1<br>		restrt:<br>		if (kpass==1) kgoto case1<br>		if (kpass==2) kgoto case2<br>		if (kpass==3) kgoto case3<br>		kgoto skip<br>		case1:<br>		ksigi = ksigi1<br>		ksigo = ksigo1<br>		kgoto skip<br>		case2:<br>		ksigi = ksigi2<br>		ksigo = ksigo2<br>		kgoto skip<br>		case3:<br>		ksigi = ksigi3<br>		ksigo = ksigo3<br>		skip:</font></p>		<p>Now we're ready to present each sample to the net, and train it. First, as was discussed, we must convert the input signal to a 16 bit binary. This binary string will be placed in the input array, starting at ibin.</p>		<p>To calculate the binary value, we first convert from the -32768 to +32767 range (a signed integer) to the 0 to 65535 range (an unsigned integer). It becomes simple to determine the binary, this value corresponds to, by knowing that in binary notation, the rightmost bit is &quot;1&quot;, whenever there's an odd number , and is reciprocally &quot;0&quot;, when the number's even. So, we just divide the value by 2 sixteen times, and at each time check if we have a zero remainder (which means a 0 bit).</p>		<p><font face="Courier New" size="2">kndx = 0<br>		ksigi = ksigi+32768<br>		loop1:<br>		kbit = (frac(ksigi/2)==0 ? 0:1)<br>		zkw kbit, ibin+kndx<br>		ksigi = int(ksigi/2)<br>		kndx = kndx+1<br>		if (kndx&lt;16) kgoto loop1</font></p>		<p>&nbsp;</p>		<p>The same method is then applied to the current output sample</p>		<p><font face="Courier New" size="2">kndx = 0<br>		ksigo = ksigo+32768<br>		loop2:<br>		kbit = (frac(ksigo/2)==0 ? 0:1)<br>		zkw kbit, ibout+kndx<br>		ksigo = int(ksigo/2)<br>		kndx = kndx+1<br>		if (kndx&lt;16) kgoto loop2</font></p>		<p>Finally, we are ready to start training. As you'll recall, hebbian learning will just use the input and output values (the binary input and output digits, we just wrote) to calculate the weight changes. To run the whole weight array, two nested loops are used, to cover all input/output combinations. At each (i,j) index pair, the input cell number <i>i</i>, and the output cell number <i>j</i>, are first zkr-ed from the input/output area, into local variables kbi and kbo, and then the weight follows the correction specified by the rule (see hebbian section). The corrected weight is then re-written to its position in the weight array.</p>		<p><font face="Courier New" size="2">train:<br>		kndi = 0<br>		loop3:<br>		kndj = 0<br>		loop4:<br>		kbi zkr ibin+kndi<br>		kbo zkr ibout+kndj<br>		kw zkr ipes+kndi*16+kndj<br>		kw = kw+((kbi*2-1)*(kbo*2-1))<br>		zkw kw, ipes+kndi*16+kndj<br>		kndj = kndj+1<br>		if (kndj&lt;16) kgoto loop4<br>		kndi = kndi+1<br>		if (kndi&lt;16) kgoto loop3</font></p>		<p>OK, we trained this pattern, now we can go to the next pass, and if there are still patterns to train, restart the loop, loading new training values into the net, and again correcting weights in accordance, till the last pattern is introduced</p>		<p><font face="Courier New" size="2"><br>		kpass = kpass+1<br>		if (kpass&lt;=inump) kgoto restrt</font></p>		<p><font face="Courier New" size="2"><br>		</font>By now, all the training patterns have been presented to the net, and the net connection weights were updated to reflect each and everyone of them. Now, we have completed the training part, and can present the net with a test (or stimulus) pattern. The resulting output of the net, is the response we went to all this trouble for, and later it will be output.</p>		<p>First, and using the now familiar method, we'll convert the stimulus signal to binary and place it in the in cells.</p>		<p><font face="Courier New" size="2">kndx = 0<br>		kstim = kstim+32768<br>		loop5:<br>		kbit = (frac(kstim/2)==0 ? 0:1)<br>		zkw kbit, ibin+kndx<br>		zkw 0, ibout+kndx<br>		kstim = int(kstim/2)<br>		kndx = kndx+1<br>		if (kndx&lt;16) kgoto loop5</font></p>		<p>Now, lets calculate the net response. This will be done in a back-and-forth fashion. First we'll calculate the output activity that corresponds to the input pattern. We do this by adding for each output cell, the complete stimulus presented to its input, times the respective weight of that connection. If this weighted sum is above 0, the neuron will fire (output 1). If it is below zero, the neuron will not fire (output 0). A tricky part is the test if kact equals 0. If that's the case, the neuron's state will remain unchanged. This is because this net evolves till it reaches a stable position, and zero activation means 'dont change'.</p>		<p>After we calculate the activation for all neurons on the output of the net, we will repeat the same process, this time forwarding the output values through the net, <i>backwards</i>, using the same weights, in order to obtain a new activation array for the input. This new set of input values, will again be forwarded to the output, and the process repeated until the net reaches a stable situation. Only then, the output of the net is taken.</p>		<p>To determine when the stable state has been reached, before each forward or backward pass, the previous activation of each neuron is saved. If the new calculated activation of the neuron is different from the saved one, an error variable (kerr) is incremented. The net is considered stable when this error variable remains with the value zero, meaning no neuron has changed its activity in the last network pass.</p>		<p><font face="Courier New" size="2"><br>		prop:<br>		kerr = 0<br>		kndi = 0<br>		loop6:<br>		kbo zkr ibout+kndi<br>		kact = 0<br>		kndj = 0<br>		loop7:<br>		kbi zkr ibin+kndj<br>		kw zkr ipes+kndj*16+kndi<br>		kact = kact+kw*kbi<br>		kndj = kndj+1<br>		if (kndj&lt;16) kgoto loop7<br>		if (kact==0) kgoto skp1<br>		kact = (kact&gt;0 ? 1:0)<br>		kerr = kerr+abs(kact-kbo)<br>		zkw kact, ibout+kndi<br>		skp1:<br>		kndi = kndi+1<br>		if (kndi&lt;16) kgoto loop6<br>		<br>		kndi = 0<br>		loop8:<br>		kbi zkr ibin+kndi<br>		kact = 0<br>		kndj = 0<br>		loop9:<br>		kbo zkr ibout+kndj<br>		kw zkr ipes+kndj*16+kndi<br>		kact = kact+kw*kbo<br>		kndj = kndj+1<br>		if (kndj&lt;16) kgoto loop9<br>		if (kact==0) kgoto skp2<br>		kact = (kact&gt;0 ? 1:0)<br>		kerr = kerr+abs(kact-kbi)<br>		zkw kact, ibin+kndi<br>		skp2:<br>		kndi = kndi+1<br>		if (kndi&lt;16) kgoto loop8<br>		<br>		if (kerr&gt;0) kgoto prop ; until stable</font></p>		<p>So now, we have the stabled net response, and its merely a matter of reconverting it to an audio value. We'll be performing the reverse of our binary encoding procedure, by means of adding each bit of the output string, multiplied by the value of the corresponding binary position (as you know, the powers of 2: 2^0, 2^1, ..., 2^15). We keep a kpow2 variable with this value, just multiplying it by two for each increasingly significant bit.</p>		<p>This will result in a value in the range 0 to 65535. We just subtract 32768 from that to get the original signed integer range.</p>		<p><font face="Courier New" size="2"><br>		kndx = 0<br>		ksigo = 0<br>		kpow2 = 1<br>		<br>		loop10:<br>		kbit zkr ibout+kndx<br>		ksigo = ksigo+kbit*kpow2<br>		kpow2 = kpow2*2<br>		kndx = kndx+1<br>		if (kndx&lt;16) kgoto loop10<br>		<br>		ksigo = ksigo-32768</font></p>		<p><font face="Courier New" size="2"><br>		</font>Since the net may occasionally mistakenly generate 'weird' values, its a good safety procedure to compress the response signal, to avoid clipping. I'll let you include your favorite routine here.</p>		<p><font face="Courier New" size="2">; compress the resulting signal<br>		;kclip table ... accepting ideas here :)<br>		<br>		</font>At last, we have our ksigo updated with response of the net, once trained, to the stimulus signal... All this trouble may seem too much for just a single sample, but its the truth. For the next sample, the net will be reinitialized (the weights cleared) so there's absolutely no learning of the signal evolution over time. Thats one of the major limitations of this instrument, since output samples become unrelated to each other, and can produce very noisy drifts. I'll try to improve on that, using better encoding strategies, and other network topologies.</p>		<p>But for now, lets just interpolate the response signal and output it,</p>		<p><font face="Courier New" size="2">aout interp ksigo<br>		out aout<br>		<br>		</font>...and thats it!</p>		<p><font face="Courier New" size="2">endin</font></p>		<p>Here's the complete orchestra file, quite nostalgically named 'Perceptron' : <a href="PERCEPTR.ORC">PERCEPTR.ORC</a></p>		<p>The only score you will need for this instrument, is one that keeps the instrument running for the sample time. Something like this:</p>		<p><font face="Courier New" size="2">i1 0 1</font></p>		<p>You can use this dummy score here: <a href="DUMMY.SCO">DUMMY.SCO</a></p>		<p><b>So what can I do with it?</b></p>		<p>This is a rather limited instrument. Nevertheless, you can experiment varying the training data, that is, the training samples you want the net to learn. Experiment changing the soundin numbers to point to your favorite samples. You can also try different stimulus patterns, even samples you havent trained the net with (notice that since we trained this net in auto-association, if you stimulate the net with one of the trained samples, the resulting response should be that same sample). Try different control rates, also, as sometimes downsampling does interesting things.</p>		<p>Later on we'll be building a better version of this instrument, so just consider this as an introductory example.&nbsp;</p>		<p>		<hr>		</p>		<dir>			<p><b>Next : </b><a href="#Anchor-training-49575">Hidden layer training (back-propagation)</a></p>			<p><b>Previous : </b><a href="HEBBIAN.HTM">Single layer training (hebbian learning)</a></p>			<p><b>Up:</b> <a href="index.html">Back to index</a>		</dir>	</body></html>