<html>	<head>		<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">		<meta name="generator" content="Adobe GoLive 4">		<title>Neural Processing using Csound [ Ref &amp; Links ]</title>		<meta name="Template" content="C:\PROGRAMAS\MICROSOFT OFFICE\OFFICE\HTML.DOT">	</head>	<body link="blue" vlink="purple" bgcolor="white">		<font size="4"><i><b>Reference section</b></i></font>		<p>This is a list of papers I found interesting and that were used as reference, or mentioned during the writing of this material.</p>		<p>F. Rosenblatt. 'The Perceptron: a probabilistic model for information storage and organization in the brain'. <i>Psychological Review</i>. 1958.</p>		<p>M. Minsky and S. Papert. 'Perceptrons'. <i>MIT Press</i>. Cambridge, 1969.</p>		<p>D. E. Rumelhart, G. E. Hinton and R. J. Williams. 'Learning Internal Representations by Error Propagation'. <i>Chapter 8 of Rumelhart and McClelland 'Parallel Distributed Processing'. MIT Press</i>. Cambridge, 1986.</p>		<p>S. E. Fahlman. 'An Empirical Study of Learning Speed in Back-Propagation Networks'.<i> Technical Report CMU-CS-88-162, Carnegie Mellon University.</i> Pittsburgh, 1988.</p>		<p>S. E. Fahlman and C. Lebiere. 'The Cascade-Correlation learning architecture'. <i>Technical Report CMU-CS-90-100, Carnegie Mellon University.</i> Pittsburgh, 1990.</p>		<p>E. M. Johannson, F. U. Dowla and D. M. Goodman. 'Backpropagation learning for multi-layer feed-forward neural networks using the conjugate gradient method'. <i>Technical Report UCRL-JC-104850, Lawrence Livermore National Laboratory.</i> 1990.</p>		<p>M. F. Moller. 'A scaled conjugate gradient algorithm for fast supervised learning'. <i>IEEE Transactions on Systems, Man and Cybernetics</i>. 1991.</p>		<p>M. Riedmiller and H. Braun.&nbsp;'A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm', <i>Proceedings of the IEEE International Conference on Neural Networks</i>. San Francisco, 1993</p>		<p>M. C. Mozer. 'Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multiscale processing'. <i>Connection Science.</i> 1994.</p>		<p>J. Gibb. 'The Back Propagation Family Album'.<i> Technical Report C/TR96-05, Macquarie University</i>. Australia 1996.&nbsp;</p>		<p>&nbsp;</p>		<p><font size="4"><i><b>Links section</b></i></font></p>		<p>There are vast resources on-line, for those who want to explore further. A mere websearch using 'neural net' should convince you of that. Many researchers and organizations are available online, and you can download most relevant papers in postscript format.</p>		<p>Here are some of my favorite sites:</p>		<p>&nbsp;</p>		<p><b>Donald Tveter's pages</b></p>		<p>This is a very comprehensive site, put together by someone who obviously loves neural networks and knows a lot about them. His 'Backpropagator's Review' gives you links to a large number of interesting papers, concerning all sorts of back-prop-based learning schemes. There's also a short back-prop explanation written by himself, most suited for beginners. Additionally you can check Tveter's interesting book 'The Pattern Recognition Basis of AI' and download some online-only chapters. The code examples he used during the research for the book are freely available. Donald Tveter also offered some helpful comments, during the writing of this material.</p>		<p>Be sure to check it all out at : <a href="http://www.dontveter.com/">http://www.dontveter.com</a></p>		<p>&nbsp;</p>		<p><b>The comp.ai.neural-nets FAQ</b></p>		<p>This is a very interesting FAQ, from the <a href="news:comp.ai.neural-nets">news:comp.ai.neural-nets</a> newsgroup. It answers most of the beginner's questions, as well as some of the advanced ones. If you want to learn more about neural nets, be sure to check this FAQ. The latest version can be found at <a href="ftp://ftp.sas.com/pub/neural/FAQ.html">ftp://ftp.sas.com/pub/neural/FAQ.html</a>.</p>		<p>&nbsp;</p>		<p><b>Ohio State Neuroprose Archive</b></p>		<p>Here you'll find a very large collection of neural related papers. You can access it by FTP at <a href="ftp://archive.cis.ohio-state.edu/pub/neuroprose/">ftp://archive.cis.ohio-state.edu/pub/neuroprose</a>, and it is also mirrored at <a href="ftp://ftp.funet.fi/pub/sci/neural/neuroprose/">ftp://ftp.funet.fi/pub/sci/neural/neuroprose</a>. Paper names start with the author name, so its easy to locate them if you know who the author is.</p>		<p>&nbsp;</p>		<p><b>Connectionist Archive FTP Site (Carnegie Mellon University) </b></p>		<p>The Carnegie Mellon University archive, at <a href="ftp://b.gp.cs.cmu.edu/afs/cs.cmu.edu/project/connect/connect-archives">ftp://b.gp.cs.cmu.edu/afs/cs.cmu.edu/project/connect/connect-archives</a>.</p>		<p>&nbsp;</p>		<p><b>Neural Information Processing Systems 1995</b></p>		<p>The papers for the NIPS'95 are found here, at <a href="http://www.cs.cmu.edu/afs/cs/project/cnbc/nips/NIPS95/Papers.html">http://www.cs.cmu.edu/afs/cs/project/cnbc/nips/NIPS95/Papers.html</a>.</p>		<p>&nbsp;</p>		<p><b>Neuroscience Search Engine</b></p>		<p>This is a very useful search engine for all kinds of neural matter, <a href="http://www.acsiom.org/nsr/neuro.html">http://www.acsiom.org/nsr/neuro.html</a>. You may also want to check Neurosciences on the Internet, at <a href="http://www.neuroguide.com/">http://www.neuroguide.com</a>.</p>		<p>&nbsp;</p>		<p><b>Dr K. Gurney's Neural Net pages</b></p>		<p>Here you'll find explanations of several network models and learning rules. This site has a good coverage of subjects, that you might find useful. The address is <a href="http://www.shef.ac.uk/psychology/gurney/notes/contents.html">http://www.shef.ac.uk/psychology/gurney/notes/contents.html</a>.</p>		<p>&nbsp;</p>		<p><b>Funet's Artificial Neural Networks Archive</b></p>		<p>A collection of neural related material, at <a href="http://www.funet.fi/pub/sci/neural">http://www.funet.fi/pub/sci/neural</a></p>		<p>&nbsp;</p>		<p><b>The Neural Processing Letters</b></p>		<p>Located at <a href="http://www.dice.ucl.ac.be/neural-nets/NPL/NPL.html">http://www.dice.ucl.ac.be/neural-nets/NPL/NPL.html</a>, this will allow you to download several interesting papers.</p>		<p>&nbsp;</p>		<p><b>Neural Nets Bibliography</b></p>		<p>Available from the University of Arizona, at <a href="http://donkey.cs.arizona.edu:1994/bib/Neural/">http://donkey.cs.arizona.edu:1994/bib/Neural</a>, and another one from the OFAI Library, at <a href="http://www.ai.univie.ac.at/oefai/nn/conn_biblio.html">http://www.ai.univie.ac.at/oefai/nn/conn_biblio.html</a>.</p>		<p>&nbsp;</p>		<p><b>BibTeX Archive</b></p>		<p>The BibTeX Collection, located at <a href="http://www.ci.tuwien.ac.at/docs/ci/bibtex_collection.html">http://www.ci.tuwien.ac.at/docs/ci/bibtex_collection.html</a> has databases of journals and conference proceedings. It offers a means of locating and citing articles, but the actual papers are not available.</p>		<p>&nbsp;</p>		<p><b>Conferences and Workshops</b></p>		<p>This page provides links to several conferences and workshops on the topic of neural networks. You can follow the links at <a href="http://www.emsl.pnl.gov:2080/proj/neuron/neural/conferences/index.html">http://www.emsl.pnl.gov:2080/proj/neuron/neural/conferences/index.html</a>.</p>		<p>&nbsp;</p>		<p>		<hr>		</p>		<dir>			<p><b>Previous:</b> <a href="WHATNX.HTM">What next?</a></p>			<p><b>Up:</b> <a href="index.html">Back to index</a>		</dir>	</body></html>