<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>

  


  
  <title>Marginalia</title>
  <link href="csoundJournal.css" rel="stylesheet" type="text/css">
</head>


<body>


<div id="wrap">
<div id="navigation"> CSOUND JOURNAL: <a href="../index.html">Home</a>
|&nbsp;<a href="index.html">Issue 9</a> </div>


<div id="header">
<h2>Marginalia</h2>

    
<h3>Designing a sound installation using Csound</h3>
Richard Bowers.<br>
saab AT richardbowers.co.uk
</div>


<div id="content">
<h2>Introduction</h2>
<h3>A Place Where Light Was Silent</h3>
<p>Thanks to a project grant from the Arts Council of Wales and the interest and
support of Cardiff School of Psychology, I was able to develop a sound
installation in the foyer of the school (Figure 1). The sound material used was
entirely derived from speech - recordings of readings through a text based on a
translation of Dante's "Inferno". The final project included visual elements in
the form of books and large-scale wall hangings and the siting of the
installation was over two locations: the school and the Washington Gallery,
Penarth. This article covers the part of the installation located at the school
and concerns itself solely with its use of Csound as the realtime sound
processing toolkit. More information about the project as a whole can be found
at <a href="http://marginalia.richardbowers.co.uk">http://marginalia.richardbowers.co.uk</a>
.</P>
<div align="center"><img border="0" src="images/foyer.JPG" alt="View of foyer" width="508" height="381">
</div>
<P align="center"><strong>Figure 1.</strong> Entrance Hall of the School of Psychology, Cardiff University</p>




<h2 align="left">
I. A Space That Listens
</h2>


<p>One aim of this project was to develop a sound installation that was
responsive to noise generated by people within the space. The noises themselves
were not being used as sound sources, but were simply to act as a trigger to
determine the parameters of the output from the loudspeakers. This was achieved
through a listening/responding model. Listening involved detecting sound levels
and responding involved taking those readings and branching conditionally to
different parts of the program to make choices. In general, I wanted the quiet
moments to generate, quiet, more static sounds while the noisy periods would generate
active, louder sounds. The range static -&gt; active covers the transition from near-frozen
sounds (a slowly evolving spectrum) moving slowly to rapidly changing sounds
which are thrown about the space more aggressively. In practice, it meant there
was a simple correlation between sound activity in the foyer caused by the
students and staff, and the activity coming from the loudspeakers.</p>

<p>Physically, the arrangement of the installation was a set of eight
loudspeakers driven from eight independent channels (Figure 2). An inner quad of speakers,
fixed to the ceiling, sat inside an outer quad. Additionally, a microphone
attached to the centre of the ceiling was able to capture the sounds within the
space in order to influence the processing of the system's audio signals. The
microphone was therefore only functioning as a sound level monitor.</p>
<p align="center"><img border="0" src="images/plan_small.jpg" alt="Plan of foyer" width="486" height="339"></p>
<P align="center"><strong>Figure 2. </strong>Physical Arrangement</p>

<p>There were two problems with this model that needed solutions:</p>
<ol>
  <li>the output of the loudspeakers would affect the sound level readings,
    thereby skewing the decision-making process;</li>
  <li>instantaneous sound level readings alone were insufficient for the purpose
    of identifying a general tendency in the ambient noise.</li>
</ol>
<p>After considering options for overcoming the first problem, including one to
deduct the output signal from the input, I chose a simpler, more elegant model
that enabled me also to a) solve the second problem and b) create a formal unity
within the piece as follows: a cycle of listening and playback where the system
would switch off playback during the listening cycle and listening would be
paused during playback.</p>

<p>This approach raised the opportunity to isolate blocks of time in which to
gather data about the sound levels in the space (Figure 3). During the listening cycle, RMS
values were accumulated and then, at the start of the playback cycle, averaged
to ascertain a general tendency in the ambient noise. At that moment, a
conditional branch based on a set of thresholds moves execution to a part of the
code to determine how playback should proceed.</p>
<p align="center"><img border="0" src="images/flowchart_small.jpg" alt="Flowchart of listening/playback model" width="452" height="508"></p>
<P align="center"><strong>Figure 3.</strong> Listening/Playback Model</p>

<p>The branching decides the following parameters: playback speed through a
group of phase vocoder analysis files; pitch transposition of those files; amplitude.</p>


<p align="center"><img border="0" src="images/orchestra_schematic_small.jpg" alt="Schematic of orchestra" width="353" height="498"></p>
<P align="center"><strong>Figure 4. </strong>Schematic of the Orchestra</p>


<h3>II. Where Words Turn to Liquid</h3>
Playback was effected through the use of phase vocoder opcodes reading analysis
files of speech recordings. These files were read at various speeds,
bi-directionally, along with some pitch shifting. With the aim being to create
an endlessly shifting river of speech sounds, moving between intelligibility and
noise, this seemed the most directly effective, and economical, model to use.
Other models were tried, including granular synthesis using banks of phonemes as
sound sources. These were successful in some respects, but lacked the fluidity
of transition between noise and intelligible speech that the phase vocoder
handles so well. (I also considered using a mix of techniques but ultimately
felt the single method gave the piece a degree of unity and robustness,
aesthetically speaking).
<p>After testing some speed/transposition combinations I selected a set of four
to use as choices within the conditional branching system.</p>
<p>The control of the pointer into the analysis files was determined, in part,
by the active branch but was also influenced by a randomized system that created
some movement around the general motion. That is the role of <i>kshuffle </i>in
the following segment:</p>
<pre>kphase	phasor	.1
krandh	randh	1, 1, iseed
krand	randi	1, .1, iseed
krand=abs(krand); make values positive
<font color="#FF0000">kshuffle</font>	randi idur1*.01*kphase, 10, iseed

kpos1	phasor	((krand/idur1)-abs(<font color="#FF0000">kshuffle</font>)*krandh)*kspeed
...</pre>
<p>where "kpos1" is the final pointer into the analysis file.</p>
Unpredictable variation to the fixed transposition value was created by the
continuous sound level reading from the microphone. Although this reading would
be contaminated by the output from the loudspeakers it nevertheless adds
unrepeatable adjustments to the transposition factor in sympathy with the
general noise levels.<pre>ktransp_mobile=gktranspindex1+(<font color="#FF0000">gkdB</font>*.005)</pre>
<p>where "gkdB" is derived from the input signal.</p>


<h3>III. Let the Internal Rhythms Translate to External Motion</h3>
One of the unifying features of the piece is the mapping of the pointer
positions to the azimuth of the quadraphonic speaker arrangement. Via Locsig/Locsend
pairs, the four channels of audio were fed to four speakers with their virtual
positions set by scaling the <i>kpos</i>n values by 360 to correspond to a full
circle.&nbsp;
<pre>aA1, aA2, aA3, aA4 locsig alivewA, <font color="#FF0000">kpos1*360</font>, kdist, kreverbsend1</pre>
<p>Values 90, 180 and 270 were added to the remaining three "kpos" variables
respectively to space each signal around the listener.</p>
<p> This method reinforces my metaphor of the mobile insofar as the sounds are
fixed in relation to each other in a spatial sense while there is a sense of
general movement about a central point.</p>

<h3>IV. Send its Shadow...its Reverberant Self</h3>
<p align="center"><img border="0" src="images/loudspeaker.JPG" alt="Ceiling mounted loudspeaker" width="492" height="369"></p>
<P align="center"><strong>Figure 5. </strong>Mounted Speaker</p>

<p>The inner quad of speakers receives the dry signals while the outer speakers
receive the output of the <i> nreverb</i> Opcodes. These are set to a ten second reverb
time. To reinforce the impression of a cavernous space the reverb is introduced
prior to and during the listening periods. This residue has less energy and therefore has
less influence over the sound level readings. Also, the buffer, <i>kpadding</i>,
is included to ensure that residual noise from the system does not influence the
readings too much.</p>

<p>Table 70, used previously to switch between listening and playback, is now
inverted and applied to the distance parameter of <i>locsig </i>(it is also
used, via <i>katt</i>, to scale the input to the reverb):</p>
<pre>kdist=((<font color="#FF0000">1-gkdistance</font>)*10)+1</pre>
<p>(+1 is added to prevent 0 which creates distortion).</p>
<p>This means that during the transition to listening the sound is pushed to the
distance to make way for the residual reverberant signal (Figure 6).</p>
<p align="center"><img border="0" src="images/table_70_small.jpg" alt="Diagram of table 70" width="543" height="343"></p>
<P align="center"><strong>Figure 6.</strong> Table 70 Diagram</p>

<h2>V. Conclusion</h2>
<h3>&quot;Part 1: How the words came into being.&quot;</h3>
<p>The installation at the university was set in a hard, reverberant space that did
not facilitate detailed positioning of sound. In retrospect, I should have produced
a work that utilised the eight speakers as independent, fixed point 'voices'.
The Washington Gallery site, on the other hand, was a small environment in which
the visitor was completely surrounded by a backcloth of a landscape behind which
the eight speakers were hidden. This was more conducive to the distribution and
movement of sounds around the listener.&nbsp;</p>

<h3>Note Regarding the Resources</h3>
<p>The .csd file is available but the .pvx files were too large to
upload/download conveniently. Therefore, to try the .csd out for yourself, you
would need to create a set of .pvx files from mono audio files. I used whispered
speech recordings of about 20-30 seconds each. Also, note that it uses eight
channels but it fairly straightforward to create a two-channel version by
changing the <i>outo</i> Opcode to <i>outs</i> and mixing the signals.</p>

<h2>Acknowledgements</h2>
I would like to acknowledge the financial support from the Arts Council of Wales
and the School of Psychology, Cardiff University, for technical assistance as
well as sponsorship-in-kind. My thanks also go to the many individuals who
helped me in various ways including lecturing and support staff at the School
and Ian Watson and Aiden Taylor.
<p>The project is detailed at <a href="http://marginalia.richardbowers.co.uk">http://marginalia.richardbowers.co.uk</a>
.
<p>Resources for this article can be found at <a href="http://marginalia.richardbowers.co.uk/resources.htm">http://marginalia.richardbowers.co.uk/resources.htm</a>
</div>


</div>


</body>
</html>
