<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="content-type" content="text/html; charset=utf-8" /><title>CSOUND JOURNAL </title><meta name="generator" content="StarOffice/OpenOffice.org XSLT (http://xml.openoffice.org/sx2ml)" /><meta name="author" content="Computer Centre" /><meta name="created" content="2008-05-23T03:58:00" /><meta name="changedby" content="Computer Centre" /><meta name="changed" content="2008-05-30T11:16:00" /><base href="." /><style type="text/css">
	@page { size: 8.5inch 11inch; margin-top: 0.7874inch; margin-bottom: 0.7874inch; margin-left: 0.7874inch; margin-right: 0.7874inch }
	table { border-collapse:collapse; border-spacing:0; empty-cells:show }
	td, th { vertical-align:top; }
	h1, h2, h3, h4, h5, h6 { clear:both }
	ol, ul { padding:0; }
	* { margin:0; }
	*.Frame { font-size:12pt; vertical-align:top; text-align:center; }
	*.Graphics { font-size:12pt; vertical-align:top; text-align:center; }
	*.OLE { font-size:12pt; vertical-align:top; text-align:center; }
	*.Caption { font-family:'Times New Roman'; font-size:12pt; margin-top:0.0835in; margin-bottom:0.0835in; font-style:italic; }
	*.Footnote { font-family:'Times New Roman'; font-size:10pt; }
	*.Heading { font-family:Arial; font-size:14pt; margin-top:0.1665in; margin-bottom:0.0835in; }
	*.Heading1 { font-family:Arial; font-size:16pt; margin-top:0.1665in; margin-bottom:0.0417in; font-weight:bold; }
	*.Heading2 { font-family:'Times New Roman'; font-size:18pt; margin-top:0.1665in; margin-bottom:0.0835in; margin-left:0in; margin-right:0in; text-indent:0inch; font-weight:bold; }
	*.Heading3 { font-family:'Times New Roman'; font-size:14pt; margin-top:0.1665in; margin-bottom:0.0835in; margin-left:0in; margin-right:0in; text-indent:0inch; font-weight:bold; }
	*.HTMLPreformatted { font-family:'Courier New'; font-size:10pt; }
	*.Index { font-family:'Times New Roman'; font-size:12pt; }
	*.List { font-family:'Times New Roman'; font-size:12pt; margin-top:0in; margin-bottom:0.0835in; }
	*.P1 { font-family:'Times New Roman'; font-size:12pt; }
	*.P10 { font-family:'Times New Roman'; font-size:18pt; margin-top:0.1665in; margin-bottom:0.0835in; margin-left:0in; margin-right:0in; text-indent:0inch; font-weight:bold; text-align:center ! important; }
	*.P11 { font-family:'Times New Roman'; font-size:9pt; margin-left:0in; margin-right:0in; line-height:99%; text-align:left ! important; text-indent:0inch; }
	*.P2 { font-family:'Times New Roman'; font-size:12pt; }
	*.P3 { font-family:'Times New Roman'; font-size:12pt; }
	*.P4 { font-family:'Times New Roman'; font-size:12pt; }
	*.P5 { font-family:'Times New Roman'; font-size:12pt; margin-top:0in; margin-bottom:0.0835in; }
	*.P6 { font-family:'Times New Roman'; font-size:12pt; margin-top:0in; margin-bottom:0.0835in; text-align:center ! important; }
	*.P7 { font-family:'Times New Roman'; font-size:12pt; margin-top:0in; margin-bottom:0.0835in; text-align:center ! important; }
	*.P8 { font-family:'Times New Roman'; font-size:12pt; margin-top:0in; margin-bottom:0.0835in; margin-left:0.4925in; margin-right:0in; text-indent:-0.4925inch; }
	*.P9 { font-family:'Times New Roman'; font-size:18pt; margin-top:0.1665in; margin-bottom:0.0835in; margin-left:0in; margin-right:0in; text-indent:0inch; font-weight:bold; }
	*.PreformattedText { font-family:'Courier New'; font-size:10pt; margin-top:0in; margin-bottom:0in; }
	*.Reference { font-family:'Times New Roman'; font-size:9pt; margin-left:0in; margin-right:0in; line-height:99%; text-align:justify ! important; text-indent:0inch; }
	*.Standard { font-family:'Times New Roman'; font-size:12pt; }
	*.Textbody { font-family:'Times New Roman'; font-size:12pt; margin-top:0in; margin-bottom:0.0835in; }
	*.BodyTextChar { font-size:12pt; }
	*.DefaultParagraphFont { }
	*.Footnoteanchor { vertical-align:sup; }
	*.FootnoteSymbol { vertical-align:sup; }
	*.Heading2Char { font-family:Arial; font-size:18pt; font-weight:bold; }
	*.HeadingChar { font-family:Arial; font-size:14pt; }
	*.Internetlink { color:#0000ff; text-decoration:underline; }
	*.StrongEmphasis { font-weight:bold; }
	*.T1 { }
	*.T2 { font-style:italic; }
	*.T3 { }
	*.T4 { font-size:12pt; }
	*.T5 { font-size:12pt; font-style:italic; }
	*.T6 { font-size:12pt; font-style:italic; }
	</style></head><body dir="ltr"><p class="P4">CSOUND JOURNAL  </p><h2 class="P10"><a name="hrtfmove_2C_hrtfstat_2C_hrtfmove2_3A_Using_the_New_HRTF_Opcodes" />hrtfmove, hrtfstat, hrtfmove2: Using the New HRTF Opcodes</h2><p class="P6"> </p><p class="P6">Brian Carty </p><p class="P7">Sound and Digital Music Technology Group, </p><p class="P6">National University of Ireland, Maynooth<br />Maynooth<br />Co. Kildare<br />Ireland<br />brian.m.carty AT nuim.ie </p><h2 class="P9"><a name="Abstract" />Abstract</h2><p class="Textbody"><span class="T1">The new Head Related Transfer Function (HRTF) opcodes introduced in csound 5.08 (March 2008) will be discussed from a practical point of view in this article. Technical aspects of the opcodes and the motivation for their development are discussed in [1]. This article will focus more on using the opcodes practically, although elements of their internal digital signal processing will be covered in context</span><span class="T1">.</span></p><p class="Textbody"><span class="Heading2Char">1. Binaural Audio</span><span class="T1"> </span></p><p class="Textbody"><span class="T1">A brief introduction to binaural audio is perhaps an apt place to begin any discussion of HRTF based audio spatialisation. Sound localisation refers to how and why we can locate sounds in our spatial environment. Receiving sound from two ears (</span><span class="T2">binaural</span><span class="T1"> hearing) is the main factor involved in sound localisation. The brain can thus compare two independent signals and establish, often very accurately, where a sound is located. The two main binaural cues for sound localisation are interaural time and intensity differences (ITD and IID respectively). Monaural cues (information from one ear) can also help with sound localisation, for example, in deciding whether a sound is in front of or behind a listener (there are no interaural cues in the median plane). The non uniform shape of the pinna filters sounds differently depending on location, which provides the main monaural cue. For more information on spatial hearing, see [2] or [3].</span></p><p class="P1">HRTFs are functions which describe how a sound is altered from source to eardrum. The HRTFs for the left and right ear for a source at a particular location thus constitute all the auditory localisation cues mentioned above. By imposing the properties of a HRTF from a specific location on an arbitrary mono input sound, the input can be artificially placed at that location/spatialised. This process is implemented using convolution.  </p><p class="Textbody"><span class="T1">Typically, HRTFs are measured at discrete points around a listener/dummy head. The data set used here was measured using a KEMAR [4] dummy at MIT [5]. Therefore, if non measured points are needed, or if a source is required to move smoothly between points, some form of interpolation is </span><span class="T1">necessary. It is important to note that the listener is essentially listening with the ears of the dummy head in this context, which can reduce the spatial quality of the outcome. Ideally, a user’s own HRTFs should be measured and used, but this is a time consuming process. Non-individualised HRTFs, as used here, give a good spatial impression.</span></p><p class="P1">HRTF interpolation is desirable in the frequency domain, for reasons of accuracy and efficiency. In the frequency domain, the magnitudes and phases of each component of the HRTF can be dealt with directly. Magnitudes can be interpolated directly, but phase is a periodic quantity, so cannot be successfully interpolated. Phase varies from 0 – 2pi, then back to 0 etc, so it is often ambiguous whether one phase value is behind another, or ahead, having moved into the next, or subsequent cycles [1].   </p><p class="Textbody"><span class="T1">A commonly used solution to the difficulty of HRTF interpolation is to break the HRTF into a minimum phase and an all pass system. This can be performed on any rational system function [6]. It has been observed that the all pass phase component of this breakdown of HRTFs is ‘nearly linear up to 10 kHz’ [7]. A linear phase all pass system can be described by a simple delay, quite a straightforward operation to implement. A key and unique property of minimum phase systems is that magnitude is related to phase. Therefore HRTF interpolation involves the extraction of the linear delay from the HRTF left and right ear pair for the location in question and the transformation of the left and right ear HRTF into minimum phase representations. Delays and magnitudes can then be interpolated, and phases derived from magnitudes. This process is quite complex and involves data preparation and transformation. Also, the assumption that the allpass phase component of the minimum phase all pass breakdown is linear is not completely accurate, although it has been found to be </span><span class="T2">adequate</span><span class="T1"> ‘for most positions [8]’. Therefore alternatives are suggested and implemented as opcodes.</span></p><p class="P1">Considerations, difficulties and processes involved in developing a software solution to HRTF based binaural processing are discussed in [9] and [3]. To summarise, the process involves applying the characteristics of the HRTF onto the sound that needs to be spatialised. The HRTF measures how the ear responds to an impulse, thus it contains information on how the ear treats all audible frequencies. The process of applying this function onto an arbitrary source sound in the frequency domain can be summarised thus: </p><ol style="margin-left:1.25cm;list-style-type:decimal; "><li class="P2" style="margin-left:0cm;"><p class="P2" style="margin-left:0.25cm;">Analyse the (ideally mono, non-spatialised) source sound to see what component frequencies make up the sound. </p></li><li class="P2" style="margin-left:0cm;"><p class="P2" style="margin-left:0.25cm;">Retrieve the measured HRTF/interpolated HRTF for the location in question. </p></li><li class="P3" style="margin-left:0cm;"><p class="P3" style="margin-left:0.25cm;"><span class="T1">Impose the HRTF onto the source sound. Boost or attenuate and delay the component frequencies that make up the source sound in the same way that the HRTF does: frequency domain </span><span class="T2">convolution</span><span class="T1">. </span></p></li><li class="P3" style="margin-left:0cm;"><p class="P3" style="margin-left:0.25cm;"><span class="T1">Play the resulting sound, in </span><span class="T2">headphones</span><span class="T1">. </span></p></li></ol><p class="Standard"><span class="T1">It is crucial that the resulting sound is played back in headphones for several reasons. Primarily, the HRTF processing has already considered the ears, so listening in loudspeakers will introduce a second set of ear processing. Loudspeaker listening is also problematic in that the signal from </span><span class="T2">both</span><span class="T1"> loudspeakers will reach </span><span class="T2">both</span><span class="T1"> ears i.e. the signal from the left loudspeaker will reach the left ear, and shortly after, the right. In headphone listening, the sound from the left headphone only reaches the left ear. Finally, the reverberant qualities of listening to the result in a room may alter the spatial image. </span></p><p class="P5">Moving sources are realisable by constantly updating the HRTF information in accordance with the source trajectory. </p><h3 class="Heading3"><a name="1.5_New_Approaches_to_HRTF_Interpolation" /><span class="T1">1.5</span><span class="T1"> </span>New Approaches to HRTF Interpolation</h3><p class="Standard">The algorithms introduced and used by the new opcodes are discussed in more detail in [1]. They both use HRTF frequency domain magnitude interpolation, but approach the problem of phase interpolation differently. The aim of the opcodes is to use the empirical/measured HRTF data directly, without complex transformations (such as minimum phase all pass decomposition and consequent phase derivation).  </p><p class="Standard">The first approach is to simply use the nearest measured phase data. The interpolated HRTF is thus composed of an interpolated magnitude spectrum taking weighted magnitudes from each of the four nearest measured HRTFs and the phase spectrum of the nearest. As our phase sensitivity does not require exact accuracy [8], this is proposed as a basic, efficient solution which allows a user to directly apply a dataset without any complex digital signal processing or data manipulation. If a source is moving, the change from one nearest measured phase to another may cause a discontinuity/click in the output. This is removed/minimised by a user defined short cross fade. For noisy, frequency rich sources, a cross fade of one spectral processing buffer (128 samples) may be enough. Narrowband sources may require more buffers.  </p><p class="Textbody">The second approach to phase interpolation is to assume the head is a sphere. A phase spectrum can thus be derived using relatively simply geometry. This crude phase spectrum is augmented by adding a frequency dependent scaling factor at the lower end of the spectrum, where phase is more important. This scaling factor is extracted from the empirical data, and is averaged across location. Therefore, a theoretically and psychoacoustically ideal phase spectrum is derived. </p><h3 class="Heading3"><a name="2.1_Preliminary_Instructions" /><span class="T1">2.1</span><span class="T1"> </span>Preliminary Instructions</h3><p class="Textbody">The opcodes need the spectral HRTF data files to run. These can be found on the csound sourceforge page: <a href="http://sourceforge.net/project/showfiles.php?group_id=81968%20"><span class="Internetlink">http://sourceforge.net/project/showfiles.php?group_id=81968</span></a> as HRTF-data5.08.zip, as well as in the samples directory of the sourceforge downloads. Each file represents the left or right data for each of the three available sampling rates. The appropriate files for the sampling rate being used should be in the current directory or the SADIR (set up an environment variable called SADIR, and give it a location, see the csound manual for details<span class="FootnoteSymbol" />). Alternatively, a path can be added to each data file parameter to the files in question. Note that examples in this article and in the manual assume that data files (at the correct sampling rate) are in the working directory or the SADIR.</p><h3 class="Heading3"><a name="2.2_hrtfmove" /><span class="T1">2.2</span><span class="T1"> </span>hrtfmove</h3><p class="Standard">hrtfmove is the first of the new opcodes. It offers phase truncation (nearest measured phase) processing or minimum phase based processing.  </p><p class="Standard"> </p><p class="PreformattedText">aleft, aright <span class="StrongEmphasis"><span class="T3">hrtfmove</span></span> asrc, kAz, kElev, ifilel, ifiler [, imode, ifade, isr]</p><p class="Standard"> </p><p class="Standard">The opcode outputs stereo left and right signals, meant for headphone reproduction, as above. hrtfmove takes a mono audio source. A k-rate source angle/azimuth and elevation are allowed (note that elevation differences are more difficult to distinguish, particularly in a non-individualised dataset). The datafiles complete the obligatory opcode arguments, as discussed above. </p><p class="Standard">The csd file below illustrates how to spatialise a mono source to a static location directly to the right of the listener: at 90 degrees. Positive angle values represent locations on the right of the listener, negative on the left. Positive elevations represent angles above the listener’s horizontal plane (max 90 degrees) and negative below (min -40 degrees). The file below outputs realtime audio, and uses 2 instruments.</p><p class="Standard">The first is a simple pluck instrument<span class="FootnoteSymbol" />, whose output is sent to a global variable, gasrc. The second uses gasrc as the input to a hrtfmove based instrument. </p><p class="Standard">The score turns on both instruments for 2 seconds. Internally, the opcode chooses the correct HRTF data to load and convolves it with the input. The datafiles are stored as spectrally analysed files, for efficiency. Therefore only the mono input needs to be Fourier analysed for the spectral convolution. Note that hrtfmove uses overlap-add convolution processing. </p><p class="Standard"> </p><p class="Standard">eg1.csd </p><p class="PreformattedText">&lt;CsoundSynthesizer&gt; </p><p class="PreformattedText">&lt;CsOptions&gt; </p><p class="PreformattedText">    ; Select flags here</p><p class="PreformattedText">    ; realtime audio out</p><p class="PreformattedText">    -o dac</p><p class="PreformattedText">    ; For non-realtime ouput leave only the line below:</p><p class="PreformattedText">    ; -o hrtf.wav</p><p class="PreformattedText">  </p><p class="PreformattedText">&lt;/CsOptions&gt; </p><p class="PreformattedText">&lt;CsInstruments&gt; </p><p class="PreformattedText"> </p><p class="PreformattedText">    sr = 44100</p><p class="PreformattedText">    kr = 4410</p><p class="PreformattedText">    ksmps = 10</p><p class="PreformattedText">    nchnls = 2</p><p class="PreformattedText"> </p><p class="PreformattedText">    gasrc init 0</p><p class="PreformattedText"> </p><p class="PreformattedText">    instr 1                ;a plucked string</p><p class="PreformattedText"> </p><p class="PreformattedText">    kamp = p4</p><p class="PreformattedText">    kcps = cpspch(p5)</p><p class="PreformattedText">    icps = cpspch(p5)</p><p class="PreformattedText"> </p><p class="PreformattedText">    a1 pluck kamp, kcps, icps, 0, 1</p><p class="PreformattedText"> </p><p class="PreformattedText">    gasrc = a1</p><p class="PreformattedText"> </p><p class="PreformattedText">    endin</p><p class="PreformattedText"> </p><p class="PreformattedText">    instr 10        ;uses output from instr1 as source</p><p class="PreformattedText"> </p><p class="PreformattedText">    aleft,aright hrtfmove gasrc, 90,0, "hrtf-44100-left.dat","hrtf-44100-right.dat"</p><p class="PreformattedText"> </p><p class="PreformattedText">    outs        aleft, aright</p><p class="PreformattedText"> </p><p class="PreformattedText">    endin</p><p class="PreformattedText"> </p><p class="PreformattedText">  </p><p class="PreformattedText">&lt;/CsInstruments&gt; </p><p class="PreformattedText">&lt;CsScore&gt; </p><p class="PreformattedText"> </p><p class="PreformattedText">    ; Play Instrument 1: a plucked string</p><p class="PreformattedText">    i1 0 2 20000 8.00</p><p class="PreformattedText"> </p><p class="PreformattedText">    ; Play Instrument 10 for 2 seconds.</p><p class="PreformattedText">    i10 0 2</p><p class="PreformattedText"> </p><p class="PreformattedText">  </p><p class="PreformattedText">&lt;/CsScore&gt; </p><p class="PreformattedText">&lt;/CsoundSynthesizer&gt; </p><p class="Standard"> </p><p class="Standard">A dynamic source is perhaps more interesting. The following csd file moves a mono, 2 second sample from in front of the listener to their right side. This csd simply opens and plays a file using soundin and uses a line to create the 0 to 90 degree trajectory. The number of Fast Fourier Transform processing buffers used to hide the clicks when the nearest measured phase data changes, as described above, defaults to 8. This works well for this example, as no clicks can be heard, and source movement is smooth and convincing. </p><p class="Standard"> </p><p class="Standard">eg2.csd </p><p class="PreformattedText">&lt;CsoundSynthesizer&gt; </p><p class="PreformattedText">&lt;CsOptions&gt; </p><p class="PreformattedText">    -o dac</p><p class="PreformattedText">&lt;/CsOptions&gt; </p><p class="PreformattedText">&lt;CsInstruments&gt; </p><p class="PreformattedText"> </p><p class="PreformattedText">    nchnls = 2</p><p class="PreformattedText"> </p><p class="PreformattedText">    instr 1                </p><p class="PreformattedText"> </p><p class="PreformattedText">    kaz line 0, p3, 90</p><p class="PreformattedText">    ain soundin "sample.wav"</p><p class="PreformattedText">    aleft,aright hrtfmove ain, kaz,0, "hrtf-44100-left.dat","hrtf-44100-right.dat"</p><p class="PreformattedText"> </p><p class="PreformattedText">    outs        aleft, aright</p><p class="PreformattedText"> </p><p class="PreformattedText">    endin</p><p class="PreformattedText"> </p><p class="PreformattedText">&lt;/CsInstruments&gt; </p><p class="PreformattedText">&lt;CsScore&gt; </p><p class="PreformattedText">  </p><p class="PreformattedText">    i1 0 2 </p><p class="PreformattedText"> </p><p class="PreformattedText">&lt;/CsScore&gt; </p><p class="PreformattedText">&lt;/CsoundSynthesizer&gt; </p><p class="Standard"> </p><p class="Standard">A more noisy sound may need only 1 buffer, for example the code snippet from eg3.csd below uses an impulsive sound as its input.  </p><p class="Standard"> </p><p class="Standard">eg3.csd </p><p class="PreformattedText">kaz line 0, p3, 90 </p><p class="PreformattedText">ain soundin "impulse.wav" </p><p class="PreformattedText">aleft,aright hrtfmove ain, kaz,0, "hrtf-44100-left.dat","hrtf-44100-right.dat", 0,1 </p><p class="Standard"> </p><p class="Standard">This code introduces the optional parameters. The first is the mode of the opcode, which will be discussed shortly, and defaults to 0, which represents phase truncation processing. The next is the amount of fades, 1 in this case works well with the impulsive sound. Indeed, with this particular frequency rich sound, longer crossfades may be undesirably audible. </p><p class="Standard">The first parameter decides the mode of the opcode: 0 for phase truncation processing, 1 for minimum phase. In minimum phase based processing, the magnitudes are interpolated as before, and the phases are derived from these magnitudes in real time. The tiny time delays representing the all pass component of the minimum phase all pass decomposition (see above) have been pre extracted and are stored internally in the opcode. So, for example, to run eg1.csd using minimum phase processing, it is simply a matter of adding this optional parameter. Note that the crossfade parameter does not effect minimum phase processing. The final parameter is sampling rate. It is important to note that the source, csd header, datafiles and opcode sr parameter should all agree. The default sr is 44100 Hz. The opcode will post a warning if the csd sr is not equal to the opcodes internal processing sr, but the source and datafile sr are up to the user to verify. The file below shows a 48000 Hz 2 second sample: sample48k.wav, being processed using the minimum phase algorithm by hrtfmove at 48000 Hz, using the 48000 Hz data, in a 48000 Hz csd file. Note that the minimum phase mode is selected, and the crossfade parameter is not in use.  </p><p class="Standard"> </p><p class="Standard">eg4.csd </p><p class="PreformattedText">&lt;CsoundSynthesizer&gt; </p><p class="PreformattedText">&lt;CsOptions&gt; </p><p class="PreformattedText">    -o dac </p><p class="PreformattedText">&lt;/CsOptions&gt; </p><p class="PreformattedText">&lt;CsInstruments&gt; </p><p class="PreformattedText"> </p><p class="PreformattedText">sr = 48000 </p><p class="PreformattedText">kr = 4800 </p><p class="PreformattedText">ksmps = 10    </p><p class="PreformattedText">nchnls = 2 </p><p class="PreformattedText"> </p><p class="PreformattedText">    instr 1                </p><p class="PreformattedText"> </p><p class="PreformattedText">kaz line 0, p3, 90 </p><p class="PreformattedText">ain soundin "sample48k.wav" </p><p class="PreformattedText">aleft,aright hrtfmove ain, kaz,0, "hrtf-48000-left.dat","hrtf-48000-right.dat", 1, 0, 48000 </p><p class="PreformattedText"> </p><p class="PreformattedText">    outs        aleft, aright</p><p class="PreformattedText"> </p><p class="PreformattedText">    endin</p><p class="PreformattedText"> </p><p class="PreformattedText">&lt;/CsInstruments&gt; </p><p class="PreformattedText">&lt;CsScore&gt; </p><p class="PreformattedText">  </p><p class="PreformattedText">    i1 0 2 </p><p class="PreformattedText"> </p><p class="PreformattedText">&lt;/CsScore&gt; </p><p class="PreformattedText">&lt;/CsoundSynthesizer&gt; </p><h3 class="Heading3"><a name="2.3_hrtfmove2_2C_hrtfstat" /><span class="T1">2.3</span><span class="T1"> </span>hrtfmove2, hrtfstat</h3><p class="Standard">The other two opcodes use the scaled spherical head model. </p><p class="Standard"> </p><p class="PreformattedText"><span class="T1">aleft, aright </span><span class="StrongEmphasis"><span class="T1">hrtfmove2</span></span><span class="T1"> asrc, kAz, kElev, ifilel, ifiler [, ioverlap, iradius, isr]</span></p><p class="PreformattedText"> </p><p class="PreformattedText"><span class="T1">aleft, aright </span><span class="StrongEmphasis"><span class="T1">hrtfstat</span></span><span class="T1"> asrc, iAz, iElev, ifilel, ifiler [, iradius, isr]</span></p><p class="Standard"> </p><p class="Standard">The reason for two opcodes is that the phase spectrum derivation requires a Short Time Fourier Transform (STFT) process to avoid noise due to the synthetic phase spectra of moving sources changing. For more on the STFT, see [10].  Thus the static version of the opcode requires significantly less processing (it uses overlap-add convolution processing, as with hrtfmove), and is isolated as a more efficient solution to static HRTF processing. The obligatory parameters are the same as hrtfmove; with the exception that hrtfstat can only take i-rate values for angle and elevation. Optional parameters are head radius and sampling rate for the static opcode, and head radius, STFT overlap and sampling rate for the dynamic opcode. Head radius is the radius value used in the geometric formula used to calculate the phase spectrum, and defaults to a value of 9.0 cm. Legal values are greater than 0 and less than or equal to 15. The STFT process, briefly, converts audio into a series of overlapping, windowed spectral frames. These frames are then processed and re-synthesised for output. The overlap parameter defines how many overlaps occur in each frame. More overlaps can capture more dynamic spectra, but increase required processing. A value of 4 is considered as a good compromise, so is the default value. Legal values are 2, 4, 8 and 16. The code snippet below, from eg5.csd uses hrtfmove2 to move the source from in front of, to the right of the user. Optional parameters (not used below) are more subtle here than in hrtfmove. For example, an overlap of factor 2 is simply too little and causes distortion, but 4 works well. Note the changes in spatialisation as the radius value is altered. For example, the spatial image is audibly affected at the extremes of the radius range. The default value of 9 cm works well for the author.   </p><p class="Standard"> </p><p class="Standard">eg5.csd </p><p class="PreformattedText">kaz line 0, p3, 90 </p><p class="PreformattedText">ain soundin "sample.wav" </p><p class="PreformattedText">aleft,aright hrtfmove2 ain, kaz,0, "hrtf-44100-left.dat","hrtf-44100-right.dat" </p><p class="Standard"> </p><p class="Standard">The code snippet from eg6.csd, below, shows use of hrtfstat. Note the i-rate azimuth and elevation values. Also, hrtfstat only has 2 optional parameters (not shown below); radius and sampling rate; overlap is omitted as STFT processing is not employed. Note also that an angle value to the extreme left of the listener can be expressed as -90 or 270 degrees (+/- any multiple of 360). </p><p class="Standard"> </p><p class="Standard">eg6.csd </p><p class="PreformattedText">ain soundin "sample.wav" </p><p class="PreformattedText">aleft,aright hrtfstat ain, -90,0, "hrtf-44100-left.dat","hrtf-44100-right.dat" </p><p class="Standard"> </p><p class="Textbody">The new opcodes serve as an update to the hrtfer opcode, introducing smooth movement and interpolation, and removing discontinuities for dynamic sources. This update is discussed in more detail in [1]. </p><h3 class="Heading3"><a name="2.4_A_More_Complex_Example" /><span class="T1">2.4</span><span class="T1"> </span>A More Complex Example</h3><p class="Standard">An interesting application of HRTF based spatialisation is the development of multi-channel binaural spatialisation tools. The main premise here is to use HRTF processing to place a source sound at a loudspeaker location relative to the listener. Thus virtual loudspeakers are situated around a listener, using binaural processing. Signals derived from a multi-channel spatialisation algorithm such as Ambisonics, Vector Based Amplitude Panning or even Wave Field Synthesis then feed these virtual loudspeakers. If, for example, a user wishes to virtualise an 8 channel Ambisonic system, 8 HRTF processes are needed to create 8 virtual sources, one at each loudspeaker location around the listener. The decoded Ambisonic signals sent to these loudspeakers are thus HRTF filtered accordingly. The user then gets the impression of being in the sweet spot of an Ambisonic configuration. eg7.csd contains a UDO which implements this Ambisonic setup in headphones. The bformdec opcode decodes an Ambisonic B format signal to a user desired loudspeaker setup. In this case, an 8 loudspeaker circular setup is used, with the loudspeaker angles (relative to the listener) defined according to the opcodes output<span class="FootnoteSymbol" />. The UDO takes a second order Ambisonic signal as its input, uses bformdec to decode this signal for Ambisonic reproduction, then HRTF filters each loudspeaker feed according to its location. All binaural virtual loudspeakers are then added for the output. </p><p class="Standard">This UDO acts as a simplified version of that available at [11]. The hrtf interpolation algorithms introduced by the new opcodes allow the possibility of non measured loudspeaker points, increasing accuracy.  </p><p class="Standard"> </p><p class="Standard">eg7.csd </p><p class="Standard"> </p><p class="PreformattedText">opcode Binambi, aa, aaaaaaaaa </p><p class="PreformattedText"> </p><p class="PreformattedText">aw, ax, ay, az, ar, as, at, au, av xin </p><p class="PreformattedText">        </p><p class="PreformattedText">; decode B format for 8 channel circle loudspeaker setup </p><p class="PreformattedText">a1, a2, a3, a4, a5, a6, a7, a8 bformdec 4, aw, ax, ay, az, ar, as, at, au, av </p><p class="PreformattedText">       </p><p class="PreformattedText">; binaurally encode each channel </p><p class="PreformattedText">; (hrtf phase truncation...) </p><p class="PreformattedText"> al1, ar1         hrtfmove  a2,  22.5,   0, "hrtf-44100-left.dat", "hrtf-44100-right.dat"</p><p class="PreformattedText"> al2, ar2         hrtfmove  a1,  67.5,   0, "hrtf-44100-left.dat", "hrtf-44100-right.dat"</p><p class="PreformattedText"> al3, ar3         hrtfmove  a8,  112.5, 0, "hrtf-44100-left.dat", "hrtf-44100-right.dat"</p><p class="PreformattedText"> al4, ar4         hrtfmove  a7,  157.5, 0, "hrtf-44100-left.dat", "hrtf-44100-right.dat"</p><p class="PreformattedText"> al5, ar5         hrtfmove  a6,  202.5, 0, "hrtf-44100-left.dat", "hrtf-44100-right.dat"</p><p class="PreformattedText"> al6, ar6         hrtfmove  a5,  247.5, 0, "hrtf-44100-left.dat", "hrtf-44100-right.dat"</p><p class="PreformattedText"> al7, ar7         hrtfmove  a4,  292.5, 0, "hrtf-44100-left.dat", "hrtf-44100-right.dat"</p><p class="PreformattedText"> al8, ar8         hrtfmove  a3,  337.5, 0, "hrtf-44100-left.dat", "hrtf-44100-right.dat"</p><p class="PreformattedText"> </p><p class="PreformattedText"> aleft = (al1+al2+al3+al4+al5+al6+al7+al8)/8</p><p class="PreformattedText"> aright = (ar1+ar2+ar3+ar4+ar5+ar6+ar7+ar8)/8</p><p class="PreformattedText">      </p><p class="PreformattedText"> ; write binaural audio out</p><p class="PreformattedText">        xout aleft, aright</p><p class="PreformattedText">endop </p><p class="Standard"> </p><p class="Textbody">The actual file eg7.csd uses this UDO. First, it encodes a signal (here the sample file read from a table in the score and played forwards, then backwards by loscil. A simple trajectory is then encoded by the bformenc opcode (note that Ambisonic coordinates use directly to the right of the user as 0 degrees, and proceed anti clockwise, by convention) and decoded using the UDO. </p><h2 class="Heading2"><a name="3._Future_Work" />3. Future Work</h2><p class="Textbody">Future work in the area will aim to further generalise this UDO. As smooth dynamic source movement is now possible, a system to allow a user to move around a virtual loudspeaker environment is feasible. A complete system will ideally allow a user to pick a multi-channel spatialisation algorithm (Ambisonics, Vector Based Amplitude Panning, Wave Field Synthesis), decide the number of, and location of loudspeakers and the reverberant qualities of their virtual listening room. Crucially, multi-channel algorithms can thus be tested for a moving listener/a user outside of the sweet spot. </p><h2 class="P9"><a name="Acknowledgements" />Acknowledgements</h2><p class="Textbody"><span class="BodyTextChar">This work is supported by the Irish Research Council for Science, Engineering and Technology: funded </span><span class="BodyTextChar">by the National Development Plan and NUI Maynooth</span>. I would also like to thank Bill Gardner and Keith Martin for making their HRTF measurements available.<span class="T1"> </span></p><ol style="margin-left:0.5cm;list-style-type:; "><li class="P11" style="margin-left:0cm;"><p class="P11" style="margin-left:-0.5cm;"><span class="T4">[1]        V. Lazzarini and B. Carty, “New Csound Opcodes for Binaural Processing”, </span><span class="T6">Proc. 6th Linux Audio Conference</span><span class="T4">, Cologne, Germany, February 2008, pp. 28-35.</span></p></li><li class="P11" style="margin-left:0cm;"><p class="P11" style="margin-left:-0.5cm;"><span class="T4">[2]        B. Moore, “An Introduction to the Psychology of Hearing”, Elsevier Academic Press, London, UK, fifth edition, 2004.</span></p></li><li class="P11" style="margin-left:0cm;"><p class="P11" style="margin-left:-0.5cm;"><span class="T4">[3]        B. Carty, “Artificial Simulation of Audio Spatialisation: Developing a Binaural System”, </span><span class="T6">Maynooth Musicology</span><span class="T4">, vol.1, pp. 271-296, March 2008. </span></p></li><li class="P11" style="margin-left:0cm;"><p class="P11" style="margin-left:-0.5cm;"><span class="T4">[4]        M. Burkhard and R. Sachs, “Anthropometric manikin for acoustic research”, </span><span class="T6">Journal of the Acoustical Society of America</span><span class="T4">, vol. 58, no. 1, pp.214-222, July 1975.</span></p></li><li class="P11" style="margin-left:0cm;"><p class="P11" style="margin-left:-0.5cm;"><span class="T4">[5]        B. Gardner and K. Martin, “HRTF Measurements of a KEMAR Dummy Head Microphone”, Available at </span><a href="http://sound.media.mit.edu/KEMAR.html"><span class="Internetlink"><span class="T4">http://sound.media.mit.edu/KEMAR.html</span></span></a><span class="T4">, Accessed May, 2008.</span></p></li><li class="P11" style="margin-left:0cm;"><p class="P11" style="margin-left:-0.5cm;"><span class="T4">[6]        A. Oppenheim, and R. Schafer, </span><span class="T6">Discrete-Time Signal Processing</span><span class="T4">, Prentice Hall, New Jersey, USA, second edition,  1999.</span></p></li><li class="P11" style="margin-left:0cm;"><p class="P11" style="margin-left:-0.5cm;"><span class="T4">[7]        S. Mehrgardt and V. Mellert, “Transformation Characteristics of the external human ear”, </span><span class="T5">Journal of the Acoustical Society of America</span><span class="T4">, vol. 61, no. 6, pp. 1567-1576, June 1977.</span></p></li><li class="P11" style="margin-left:0cm;"><p class="P11" style="margin-left:-0.5cm;"><span class="T4">[8]        A. Kulkarni, S. Isabelle and H. Colburn, “Sensitivity of Human Subjects to Head-Related Transfer-Function Phase Spectra”, </span><span class="T6">Journal of the Acoustical Society of America</span><span class="T4">, vol. 105, no. 5, pp. 2821-2840, May 1999.</span></p></li><li class="P11" style="margin-left:0cm;"><p class="P11" style="margin-left:-0.5cm;"><span class="T4">[9]        D. Begault, 3-D Sound for Virtual Reality and Multimedia, AP Professional, London, UK, 1994.</span></p></li><li class="P11" style="margin-left:0cm;"><p class="P11" style="margin-left:-0.5cm;"><span class="T4">[10]        F. R. Moore, </span><span class="T6">Elements of Computer Music</span><span class="T4">, Prentice-Hall, New Jersey, USA, 1990.</span></p></li><li class="P11" style="margin-left:0cm;"><p class="P11" style="margin-left:-0.5cm;"><span class="T4">[11]        J. Hoffman, </span><a href="http://www.sonicarchitecture.de/"><span class="Internetlink"><span class="T4">http://www.sonicarchitecture.de/</span></span></a><span class="T4">, accessed May, 2008.</span></p></li></ol><p class="P8">[12]        <a href="http://www.csounds.com/"><span class="Internetlink">www.csounds.com</span></a></p></body></html>